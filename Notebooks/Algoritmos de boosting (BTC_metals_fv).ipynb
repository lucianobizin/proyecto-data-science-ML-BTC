{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fecha de entrega: 24/09/2022 (corresponde a las clases 23/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BTC_metals_fv** (data set \"BTC_metals\" reducido según las conclusiones referidas en el punto 7 del documento \"Primera entrega\")\n",
    "\n",
    "- **Date**: fecha en YYYY-MM-DD (object [ns])\n",
    "- **Price**: precio de cierre de BTC en el día de la fecha (USD) (float) (Variable numérica continua)\n",
    "- **Open**: precio de apertura de BTC en el día de la fecha (USD) (float) (Variable numérica continua)\n",
    "- **High**: precio más alto de BTC en el día de la fecha (USD) (float) (Variable numérica continua)\n",
    "- **Low**: precio más bajo de BTC en el día de la fecha (USD) (float) (Variable numérica continua)\n",
    "- **Vol.**: volumen de BTC (Número de intercambios) en el día de la fecha (float) (Variable numérica continua)\n",
    "- **Percentage_diff**: diferencia porcentual del precio de BTC en la fecha [x+1] con respecto a la fecha [x] (float) (Variable numérica continua) \n",
    "- **Target**: 1 indica que en el día de la fecha el precio de BTC subió, y 0 que el precio bajó (float) (Se la tratará como variable categórica)\n",
    "- **Price_gold**: indica el precio de cierre del oro en el día de la fecha (float) (USD) (variable numérica continua)\n",
    "- **Perc_diff_gold**: diferencial porcentual del precio del oro de la fecha [x+1] con respecto a la fecha [x] (float) (variable numérica continua)\n",
    "- **Price_copper**: indica el precio de cierre del cobre en el día de la fecha (float) (USD) (variable numérica continua)\n",
    "- **Perc_diff_copper**:  diferencial porcentual del precio del cobre en la fecha [x+1] con respecto a la fecha [x] (float) (variable numérica continua)\n",
    "- **Price_platinum**: indica el precio de cierre del platino en el día de la fecha (float) (USD) (variable numérica continua)\n",
    "- **Perc_diff_platinum**:  diferencial porcentual del precio del platino en la fecha [x+1] con respecto a la fecha [x] (float) (variable numérica continua)\n",
    "- **Price_palladium**: indica el precio de cierre del paladio en el día de la fecha (float) (USD) (variable numérica continua)\n",
    "- **Perc_diff_palladium**: diferencial porcentual del precio del paladio en la fecha [x+1] con respecto a la fecha [x] (float) (variable numérica continua)\n",
    "- **Trend**: indica la tendencia de la serie de tiempo en relación con la variable [Price] (float) (variable numérica continua)\n",
    "- **Residuals**: indica los residuales de la serie de tiempo en relación con la variable [Price] (float) (variable numérica continua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucia\\anaconda3\\envs\\acjup\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "C:\\Users\\lucia\\anaconda3\\envs\\acjup\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "import statsmodels.api as sm\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdriveColabPath(sharing_url):\n",
    "  file_id=sharing_url.split('/')[-2]\n",
    "  dwn_url='https://drive.google.com/uc?id=' + file_id\n",
    "  return dwn_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adquiriendo BTC_metals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga la base de datos de BTC_metals_fv desde la carpeta compartida en el Google Drive\n",
    "sharing_url = \"https://drive.google.com/file/d/1lU7RCERkjbbrHv8OiQb2EI4PIpk_3BW0/view?usp=sharing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Percentage_diff</th>\n",
       "      <th>Target</th>\n",
       "      <th>Price_gold</th>\n",
       "      <th>Perc_diff_gold</th>\n",
       "      <th>Price_copper</th>\n",
       "      <th>Perc_diff_copper</th>\n",
       "      <th>Price_aluminium</th>\n",
       "      <th>Perc_diff_aluminium</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Residuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-06-23</td>\n",
       "      <td>591.2</td>\n",
       "      <td>603.6</td>\n",
       "      <td>604.7</td>\n",
       "      <td>582.9</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1318.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.146</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1892.5</td>\n",
       "      <td>0.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-06-24</td>\n",
       "      <td>588.8</td>\n",
       "      <td>591.2</td>\n",
       "      <td>595.7</td>\n",
       "      <td>585.4</td>\n",
       "      <td>3080.0</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1320.9</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.147</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1901.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-06-25</td>\n",
       "      <td>568.5</td>\n",
       "      <td>588.8</td>\n",
       "      <td>589.6</td>\n",
       "      <td>566.9</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>-3.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1322.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.160</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1903.5</td>\n",
       "      <td>0.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-06-26</td>\n",
       "      <td>582.7</td>\n",
       "      <td>568.5</td>\n",
       "      <td>582.7</td>\n",
       "      <td>565.8</td>\n",
       "      <td>4150.0</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1316.1</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>3.161</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>590.300000</td>\n",
       "      <td>15.543527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-06-27</td>\n",
       "      <td>602.2</td>\n",
       "      <td>582.7</td>\n",
       "      <td>603.9</td>\n",
       "      <td>580.1</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>3.149</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>1885.0</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>596.571429</td>\n",
       "      <td>20.802061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Price   Open   High    Low    Vol.  Percentage_diff  Target  \\\n",
       "0  2014-06-23  591.2  603.6  604.7  582.9  2650.0            -2.06     0.0   \n",
       "1  2014-06-24  588.8  591.2  595.7  585.4  3080.0            -0.41     0.0   \n",
       "2  2014-06-25  568.5  588.8  589.6  566.9  3090.0            -3.45     0.0   \n",
       "3  2014-06-26  582.7  568.5  582.7  565.8  4150.0             2.51     1.0   \n",
       "4  2014-06-27  602.2  582.7  603.9  580.1  3390.0             3.35     1.0   \n",
       "\n",
       "   Price_gold  Perc_diff_gold  Price_copper  Perc_diff_copper  \\\n",
       "0      1318.0            0.14         3.146              0.90   \n",
       "1      1320.9            0.22         3.147              0.03   \n",
       "2      1322.2            0.10         3.160              0.41   \n",
       "3      1316.1           -0.46         3.161              0.03   \n",
       "4      1319.0            0.22         3.149             -0.38   \n",
       "\n",
       "   Price_aluminium  Perc_diff_aluminium       Trend  Residuals  \n",
       "0           1892.5                 0.21         NaN        NaN  \n",
       "1           1901.0                 0.45         NaN        NaN  \n",
       "2           1903.5                 0.13         NaN        NaN  \n",
       "3           1898.0                -0.29  590.300000  15.543527  \n",
       "4           1885.0                -0.68  596.571429  20.802061  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwn_url=gdriveColabPath(sharing_url)\n",
    "BTC_metals_fv_df = pd.read_csv(dwn_url, sep=\";\", decimal=\".\")\n",
    "BTC_metals_fv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis pre-modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2949, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Registros y columnas\n",
    "BTC_metals_fv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Price', 'Open', 'High', 'Low', 'Vol.', 'Percentage_diff',\n",
       "       'Target', 'Price_gold', 'Perc_diff_gold', 'Price_copper',\n",
       "       'Perc_diff_copper', 'Price_aluminium', 'Perc_diff_aluminium', 'Trend',\n",
       "       'Residuals'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre de columnas\n",
    "BTC_metals_fv_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                    object\n",
       "Price                  float64\n",
       "Open                   float64\n",
       "High                   float64\n",
       "Low                    float64\n",
       "Vol.                   float64\n",
       "Percentage_diff        float64\n",
       "Target                 float64\n",
       "Price_gold             float64\n",
       "Perc_diff_gold         float64\n",
       "Price_copper           float64\n",
       "Perc_diff_copper       float64\n",
       "Price_aluminium        float64\n",
       "Perc_diff_aluminium    float64\n",
       "Trend                  float64\n",
       "Residuals              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tipos de datos de las variables\n",
    "BTC_metals_fv_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                   2949\n",
       "Price                  2949\n",
       "Open                   2949\n",
       "High                   2949\n",
       "Low                    2949\n",
       "Vol.                   2949\n",
       "Percentage_diff        2949\n",
       "Target                 2949\n",
       "Price_gold             2949\n",
       "Perc_diff_gold         2949\n",
       "Price_copper           2949\n",
       "Perc_diff_copper       2949\n",
       "Price_aluminium        2949\n",
       "Perc_diff_aluminium    2949\n",
       "Trend                  2943\n",
       "Residuals              2943\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columnas y cantidad de registros\n",
    "BTC_metals_fv_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                   0\n",
       "Price                  0\n",
       "Open                   0\n",
       "High                   0\n",
       "Low                    0\n",
       "Vol.                   0\n",
       "Percentage_diff        0\n",
       "Target                 0\n",
       "Price_gold             0\n",
       "Perc_diff_gold         0\n",
       "Price_copper           0\n",
       "Perc_diff_copper       0\n",
       "Price_aluminium        0\n",
       "Perc_diff_aluminium    0\n",
       "Trend                  6\n",
       "Residuals              6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores NaN\n",
    "BTC_metals_fv_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "\n",
    "Se observaron 12 NaN en \"BTC_metals_fv.csv\" => 6 valores NaN en la variable [Trend] y 6 valores NaN en la variable [Residuals]. Esto implica la necesidad de tomar una decisión con respecto a tipo de valores (instancia previa al modelado). \n",
    "\n",
    "Se descarta rellenar los NaN con promedios de valores previos y/o posteriores por varias razones:\n",
    "\n",
    "1. En ambas variables, 3 de los 6 valores NaN se encuentran al principio de la serie, y los restantes 3 al final de la misma. Esto significa que no se puede hacer un promedio con valores anteriores y posteriores a los valores NaN sino solo con anteriores o posteriores según el caso. Proceder con un promedio con los datos anteriores o posteriores en una serie de tiempo supone un problema extra (¿cuántos valores hacia atrás o hacia adelante se deben tomar?, ¿cuál es el grado de autocorrelación con los valores anteriores o posteriores?, entre otros). \n",
    "\n",
    "\n",
    "2. En la variable [Residuals] hay valores positivos y negativos en los registros anteriores y posteriores inmediatos según el caso. Esto también añade un problema extra (¿qué signo se debería utilizar si se rellenan esos casilleros?).\n",
    "\n",
    "\n",
    "3. La tercera opción es dejar que los modelos se encarguen de procesar y analizar los NaN (lo que parece la opción más coherente en esta instancia), y si esto no funciona por las características de los modelos (error crítico/insalvable), por último, se decidirá eliminar esos registros. Al no tratarse de un problema de regresión lineal del valor que tomaría el precio de BTC en el registro siguiente sino un problema de clasificación, la eliminación de los 6 valores NaN no afectaría en demasía la predicción del modelo a pesar de la autocorrelación referida en el punto 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultimas manipulaciones pre-modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se reindexa el data set que se modelará a fin de que la variable [Date] funcione como nuevo índice de la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una copia de BTC_metals_fv_df con nombre BTC_metals y se la reindexa con la variable [Date] como nuevo índice\n",
    "BTC_metals = BTC_metals_fv_df.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Percentage_diff</th>\n",
       "      <th>Target</th>\n",
       "      <th>Price_gold</th>\n",
       "      <th>Perc_diff_gold</th>\n",
       "      <th>Price_copper</th>\n",
       "      <th>Perc_diff_copper</th>\n",
       "      <th>Price_aluminium</th>\n",
       "      <th>Perc_diff_aluminium</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Residuals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-07-15</th>\n",
       "      <td>20825.1</td>\n",
       "      <td>20586.1</td>\n",
       "      <td>21178.1</td>\n",
       "      <td>20393.4</td>\n",
       "      <td>164670.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1703.60</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>20787.614286</td>\n",
       "      <td>52.659204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-16</th>\n",
       "      <td>21209.9</td>\n",
       "      <td>20825.2</td>\n",
       "      <td>21561.3</td>\n",
       "      <td>20484.4</td>\n",
       "      <td>136890.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1703.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21370.371429</td>\n",
       "      <td>-160.520267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-17</th>\n",
       "      <td>20785.6</td>\n",
       "      <td>21209.8</td>\n",
       "      <td>21654.4</td>\n",
       "      <td>20755.2</td>\n",
       "      <td>132810.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1703.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18</th>\n",
       "      <td>22525.8</td>\n",
       "      <td>20785.6</td>\n",
       "      <td>22714.9</td>\n",
       "      <td>20770.6</td>\n",
       "      <td>279720.0</td>\n",
       "      <td>8.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1710.20</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.3167</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-19</th>\n",
       "      <td>23410.2</td>\n",
       "      <td>22529.3</td>\n",
       "      <td>23757.3</td>\n",
       "      <td>21581.8</td>\n",
       "      <td>308910.0</td>\n",
       "      <td>3.93</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1708.05</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.2965</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>2386.0</td>\n",
       "      <td>-2.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Price     Open     High      Low      Vol.  Percentage_diff  \\\n",
       "Date                                                                        \n",
       "2022-07-15  20825.1  20586.1  21178.1  20393.4  164670.0             1.16   \n",
       "2022-07-16  21209.9  20825.2  21561.3  20484.4  136890.0             1.85   \n",
       "2022-07-17  20785.6  21209.8  21654.4  20755.2  132810.0            -2.00   \n",
       "2022-07-18  22525.8  20785.6  22714.9  20770.6  279720.0             8.37   \n",
       "2022-07-19  23410.2  22529.3  23757.3  21581.8  308910.0             3.93   \n",
       "\n",
       "            Target  Price_gold  Perc_diff_gold  Price_copper  \\\n",
       "Date                                                           \n",
       "2022-07-15     1.0     1703.60           -0.13        3.2455   \n",
       "2022-07-16     1.0     1703.60            0.00        3.2455   \n",
       "2022-07-17     0.0     1703.60            0.00        3.2455   \n",
       "2022-07-18     1.0     1710.20            0.39        3.3167   \n",
       "2022-07-19     1.0     1708.05           -0.13        3.2965   \n",
       "\n",
       "            Perc_diff_copper  Price_aluminium  Perc_diff_aluminium  \\\n",
       "Date                                                                 \n",
       "2022-07-15              0.49           2344.0                 0.69   \n",
       "2022-07-16              0.00           2344.0                 0.00   \n",
       "2022-07-17              0.00           2344.0                 0.00   \n",
       "2022-07-18              2.18           2435.0                 3.88   \n",
       "2022-07-19             -0.61           2386.0                -2.01   \n",
       "\n",
       "                   Trend   Residuals  \n",
       "Date                                  \n",
       "2022-07-15  20787.614286   52.659204  \n",
       "2022-07-16  21370.371429 -160.520267  \n",
       "2022-07-17           NaN         NaN  \n",
       "2022-07-18           NaN         NaN  \n",
       "2022-07-19           NaN         NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTC_metals.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que en la práctica real solo se tiene hasta los valores del registro anterior al que se quiere predecir se intenta hacer un shift (-1) a la variable [Target]. Se entiende que así (en caso de que el modelo prediga con previsibilidad suficiente) se podría estar seguro de que después vendría el valor de [Target] original sin shift (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera un back up\n",
    "BTC_metals_2 = BTC_metals.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza el shift \n",
    "BTC_metals_2 [\"Target\"] = BTC_metals_2 [\"Target\"].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Percentage_diff</th>\n",
       "      <th>Target</th>\n",
       "      <th>Price_gold</th>\n",
       "      <th>Perc_diff_gold</th>\n",
       "      <th>Price_copper</th>\n",
       "      <th>Perc_diff_copper</th>\n",
       "      <th>Price_aluminium</th>\n",
       "      <th>Perc_diff_aluminium</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Residuals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-07-15</th>\n",
       "      <td>20825.1</td>\n",
       "      <td>20586.1</td>\n",
       "      <td>21178.1</td>\n",
       "      <td>20393.4</td>\n",
       "      <td>164670.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1703.60</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>20787.614286</td>\n",
       "      <td>52.659204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-16</th>\n",
       "      <td>21209.9</td>\n",
       "      <td>20825.2</td>\n",
       "      <td>21561.3</td>\n",
       "      <td>20484.4</td>\n",
       "      <td>136890.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1703.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21370.371429</td>\n",
       "      <td>-160.520267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-17</th>\n",
       "      <td>20785.6</td>\n",
       "      <td>21209.8</td>\n",
       "      <td>21654.4</td>\n",
       "      <td>20755.2</td>\n",
       "      <td>132810.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1703.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18</th>\n",
       "      <td>22525.8</td>\n",
       "      <td>20785.6</td>\n",
       "      <td>22714.9</td>\n",
       "      <td>20770.6</td>\n",
       "      <td>279720.0</td>\n",
       "      <td>8.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1710.20</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.3167</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-19</th>\n",
       "      <td>23410.2</td>\n",
       "      <td>22529.3</td>\n",
       "      <td>23757.3</td>\n",
       "      <td>21581.8</td>\n",
       "      <td>308910.0</td>\n",
       "      <td>3.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1708.05</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.2965</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>2386.0</td>\n",
       "      <td>-2.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Price     Open     High      Low      Vol.  Percentage_diff  \\\n",
       "Date                                                                        \n",
       "2022-07-15  20825.1  20586.1  21178.1  20393.4  164670.0             1.16   \n",
       "2022-07-16  21209.9  20825.2  21561.3  20484.4  136890.0             1.85   \n",
       "2022-07-17  20785.6  21209.8  21654.4  20755.2  132810.0            -2.00   \n",
       "2022-07-18  22525.8  20785.6  22714.9  20770.6  279720.0             8.37   \n",
       "2022-07-19  23410.2  22529.3  23757.3  21581.8  308910.0             3.93   \n",
       "\n",
       "            Target  Price_gold  Perc_diff_gold  Price_copper  \\\n",
       "Date                                                           \n",
       "2022-07-15     1.0     1703.60           -0.13        3.2455   \n",
       "2022-07-16     0.0     1703.60            0.00        3.2455   \n",
       "2022-07-17     1.0     1703.60            0.00        3.2455   \n",
       "2022-07-18     1.0     1710.20            0.39        3.3167   \n",
       "2022-07-19     NaN     1708.05           -0.13        3.2965   \n",
       "\n",
       "            Perc_diff_copper  Price_aluminium  Perc_diff_aluminium  \\\n",
       "Date                                                                 \n",
       "2022-07-15              0.49           2344.0                 0.69   \n",
       "2022-07-16              0.00           2344.0                 0.00   \n",
       "2022-07-17              0.00           2344.0                 0.00   \n",
       "2022-07-18              2.18           2435.0                 3.88   \n",
       "2022-07-19             -0.61           2386.0                -2.01   \n",
       "\n",
       "                   Trend   Residuals  \n",
       "Date                                  \n",
       "2022-07-15  20787.614286   52.659204  \n",
       "2022-07-16  21370.371429 -160.520267  \n",
       "2022-07-17           NaN         NaN  \n",
       "2022-07-18           NaN         NaN  \n",
       "2022-07-19           NaN         NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTC_metals_2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que ninguno de los modelos que se ejecutarán debajo no admiten valores NaN en la variable target se procede a la eliminación del último registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BTC_metals_2 = BTC_metals_2.drop (index=\"2022-07-19\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Vol.</th>\n",
       "      <th>Percentage_diff</th>\n",
       "      <th>Target</th>\n",
       "      <th>Price_gold</th>\n",
       "      <th>Perc_diff_gold</th>\n",
       "      <th>Price_copper</th>\n",
       "      <th>Perc_diff_copper</th>\n",
       "      <th>Price_aluminium</th>\n",
       "      <th>Perc_diff_aluminium</th>\n",
       "      <th>Trend</th>\n",
       "      <th>Residuals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-07-14</th>\n",
       "      <td>20586.0</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>20862.2</td>\n",
       "      <td>19664.9</td>\n",
       "      <td>205280.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1705.8</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>3.2298</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>2328.0</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>20421.528571</td>\n",
       "      <td>187.614955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-15</th>\n",
       "      <td>20825.1</td>\n",
       "      <td>20586.1</td>\n",
       "      <td>21178.1</td>\n",
       "      <td>20393.4</td>\n",
       "      <td>164670.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1703.6</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>20787.614286</td>\n",
       "      <td>52.659204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-16</th>\n",
       "      <td>21209.9</td>\n",
       "      <td>20825.2</td>\n",
       "      <td>21561.3</td>\n",
       "      <td>20484.4</td>\n",
       "      <td>136890.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1703.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21370.371429</td>\n",
       "      <td>-160.520267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-17</th>\n",
       "      <td>20785.6</td>\n",
       "      <td>21209.8</td>\n",
       "      <td>21654.4</td>\n",
       "      <td>20755.2</td>\n",
       "      <td>132810.0</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1703.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.2455</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18</th>\n",
       "      <td>22525.8</td>\n",
       "      <td>20785.6</td>\n",
       "      <td>22714.9</td>\n",
       "      <td>20770.6</td>\n",
       "      <td>279720.0</td>\n",
       "      <td>8.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1710.2</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.3167</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Price     Open     High      Low      Vol.  Percentage_diff  \\\n",
       "Date                                                                        \n",
       "2022-07-14  20586.0  20250.0  20862.2  19664.9  205280.0             1.66   \n",
       "2022-07-15  20825.1  20586.1  21178.1  20393.4  164670.0             1.16   \n",
       "2022-07-16  21209.9  20825.2  21561.3  20484.4  136890.0             1.85   \n",
       "2022-07-17  20785.6  21209.8  21654.4  20755.2  132810.0            -2.00   \n",
       "2022-07-18  22525.8  20785.6  22714.9  20770.6  279720.0             8.37   \n",
       "\n",
       "            Target  Price_gold  Perc_diff_gold  Price_copper  \\\n",
       "Date                                                           \n",
       "2022-07-14     1.0      1705.8           -1.71        3.2298   \n",
       "2022-07-15     1.0      1703.6           -0.13        3.2455   \n",
       "2022-07-16     0.0      1703.6            0.00        3.2455   \n",
       "2022-07-17     1.0      1703.6            0.00        3.2455   \n",
       "2022-07-18     1.0      1710.2            0.39        3.3167   \n",
       "\n",
       "            Perc_diff_copper  Price_aluminium  Perc_diff_aluminium  \\\n",
       "Date                                                                 \n",
       "2022-07-14             -2.54           2328.0                -1.48   \n",
       "2022-07-15              0.49           2344.0                 0.69   \n",
       "2022-07-16              0.00           2344.0                 0.00   \n",
       "2022-07-17              0.00           2344.0                 0.00   \n",
       "2022-07-18              2.18           2435.0                 3.88   \n",
       "\n",
       "                   Trend   Residuals  \n",
       "Date                                  \n",
       "2022-07-14  20421.528571  187.614955  \n",
       "2022-07-15  20787.614286   52.659204  \n",
       "2022-07-16  21370.371429 -160.520267  \n",
       "2022-07-17           NaN         NaN  \n",
       "2022-07-18           NaN         NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTC_metals_2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que la mayoría de los modelos no admiten valores NaN, se eliminan los registros de este tipo.\n",
    "\n",
    "Por ejemplo, **error**: AdaBoostClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BTC_metals_2 = BTC_metals_2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price                  0\n",
       "Open                   0\n",
       "High                   0\n",
       "Low                    0\n",
       "Vol.                   0\n",
       "Percentage_diff        0\n",
       "Target                 0\n",
       "Price_gold             0\n",
       "Perc_diff_gold         0\n",
       "Price_copper           0\n",
       "Perc_diff_copper       0\n",
       "Price_aluminium        0\n",
       "Perc_diff_aluminium    0\n",
       "Trend                  0\n",
       "Residuals              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTC_metals_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...Recordando que...**\n",
    "\n",
    "*Objetivo general del proyecto*\n",
    "\n",
    "I.\tDiseñar diversos modelos de machine learning capaces de predecir con cierto grado de previsibilidad positiva (>50/55%) el dinamismo del precio de Bitcoin en el corto plazo, es decir, si el precio de este criptoactivo subirá o bajará en la temporalidad estudiada (+1 día)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación**:\n",
    "\n",
    "- En lo que sigue se procederá a realizar varios modelos supervisados de clasificación de tipo \"decision tree\". \n",
    "- Estos serán configurados con diversos hiperparámetros a fin de encontrar el resultado más óptimo para este data set.\n",
    "- La columna [Target] de BTC_metals_2 será considerada su variable target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan las librearía necesarias\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen X e y\n",
    "X = BTC_metals_2.drop (['Target'],axis=1)\n",
    "y = BTC_metals_2 ['Target']\n",
    "\n",
    "# Dividimos los datos en Train y Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el modelo AdaBoostClassifier\n",
    "ada = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye la grilla de GridSerarchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definelos hyperparámetros\n",
    "param_grid = {'n_estimators': np.arange(1, 100, 1), \n",
    "              'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'random_state': np.arange (1,1000,1)  \n",
    "             }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Se utiliza la grilla definida anteriormente y se agrega Cross validation con k=5 \n",
    "model = RandomizedSearchCV(ada, param_grid, verbose=3, n_iter=100, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV 1/5] END learning_rate=10, n_estimators=50, random_state=894;, score=0.380 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=50, random_state=894;, score=0.295 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=50, random_state=894;, score=0.348 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=50, random_state=894;, score=0.310 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=50, random_state=894;, score=0.343 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=100, n_estimators=1, random_state=545;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, n_estimators=1, random_state=545;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, n_estimators=1, random_state=545;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, n_estimators=1, random_state=545;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, n_estimators=1, random_state=545;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=96, random_state=178;, score=0.380 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=96, random_state=178;, score=0.295 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=96, random_state=178;, score=0.348 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=96, random_state=178;, score=0.310 total time=   0.4s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=96, random_state=178;, score=0.343 total time=   0.4s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=93, random_state=758;, score=0.620 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=93, random_state=758;, score=0.692 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=93, random_state=758;, score=0.650 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=93, random_state=758;, score=0.660 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=93, random_state=758;, score=0.655 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=55, random_state=694;, score=0.611 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=55, random_state=694;, score=0.703 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=55, random_state=694;, score=0.652 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=55, random_state=694;, score=0.677 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=55, random_state=694;, score=0.664 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=79, random_state=380;, score=0.603 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=79, random_state=380;, score=0.677 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=79, random_state=380;, score=0.639 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=79, random_state=380;, score=0.641 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=79, random_state=380;, score=0.653 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=47, random_state=692;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=47, random_state=692;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=47, random_state=692;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=47, random_state=692;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=47, random_state=692;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=42, random_state=173;, score=0.611 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=42, random_state=173;, score=0.686 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=42, random_state=173;, score=0.639 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=42, random_state=173;, score=0.643 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=42, random_state=173;, score=0.647 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.01, n_estimators=41, random_state=636;, score=0.616 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.01, n_estimators=41, random_state=636;, score=0.709 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.01, n_estimators=41, random_state=636;, score=0.660 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.01, n_estimators=41, random_state=636;, score=0.684 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.01, n_estimators=41, random_state=636;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=100, n_estimators=71, random_state=666;, score=0.384 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, n_estimators=71, random_state=666;, score=0.293 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, n_estimators=71, random_state=666;, score=0.342 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, n_estimators=71, random_state=666;, score=0.312 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, n_estimators=71, random_state=666;, score=0.338 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=53, random_state=855;, score=0.611 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=53, random_state=855;, score=0.686 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=53, random_state=855;, score=0.637 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=53, random_state=855;, score=0.645 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=53, random_state=855;, score=0.643 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.0001, n_estimators=2, random_state=189;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.0001, n_estimators=2, random_state=189;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.0001, n_estimators=2, random_state=189;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.0001, n_estimators=2, random_state=189;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.0001, n_estimators=2, random_state=189;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=44, random_state=485;, score=0.611 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=44, random_state=485;, score=0.690 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=44, random_state=485;, score=0.650 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=44, random_state=485;, score=0.645 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=44, random_state=485;, score=0.653 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=94, random_state=171;, score=0.616 total time=   0.4s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=94, random_state=171;, score=0.709 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=94, random_state=171;, score=0.660 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=94, random_state=171;, score=0.684 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=94, random_state=171;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.01, n_estimators=70, random_state=408;, score=0.616 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, n_estimators=70, random_state=408;, score=0.709 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, n_estimators=70, random_state=408;, score=0.660 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, n_estimators=70, random_state=408;, score=0.684 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, n_estimators=70, random_state=408;, score=0.662 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.0001, n_estimators=59, random_state=369;, score=0.616 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.0001, n_estimators=59, random_state=369;, score=0.709 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.0001, n_estimators=59, random_state=369;, score=0.660 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.0001, n_estimators=59, random_state=369;, score=0.684 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.0001, n_estimators=59, random_state=369;, score=0.662 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=22, random_state=547;, score=0.620 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=22, random_state=547;, score=0.694 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=22, random_state=547;, score=0.648 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=22, random_state=547;, score=0.669 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=22, random_state=547;, score=0.638 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=100, n_estimators=38, random_state=538;, score=0.384 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, n_estimators=38, random_state=538;, score=0.293 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, n_estimators=38, random_state=538;, score=0.342 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, n_estimators=38, random_state=538;, score=0.312 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, n_estimators=38, random_state=538;, score=0.338 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=64, random_state=422;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=64, random_state=422;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=64, random_state=422;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=64, random_state=422;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=64, random_state=422;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=47, random_state=100;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=47, random_state=100;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=47, random_state=100;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=47, random_state=100;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=47, random_state=100;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=23, random_state=639;, score=0.624 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=23, random_state=639;, score=0.699 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=23, random_state=639;, score=0.641 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=23, random_state=639;, score=0.665 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=23, random_state=639;, score=0.640 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=46, random_state=485;, score=0.380 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=46, random_state=485;, score=0.295 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=46, random_state=485;, score=0.348 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=46, random_state=485;, score=0.310 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=46, random_state=485;, score=0.343 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.01, n_estimators=97, random_state=805;, score=0.616 total time=   0.4s\n",
      "[CV 2/5] END learning_rate=0.01, n_estimators=97, random_state=805;, score=0.709 total time=   0.4s\n",
      "[CV 3/5] END learning_rate=0.01, n_estimators=97, random_state=805;, score=0.660 total time=   0.4s\n",
      "[CV 4/5] END learning_rate=0.01, n_estimators=97, random_state=805;, score=0.684 total time=   0.4s\n",
      "[CV 5/5] END learning_rate=0.01, n_estimators=97, random_state=805;, score=0.662 total time=   0.4s\n",
      "[CV 1/5] END learning_rate=0.01, n_estimators=94, random_state=806;, score=0.616 total time=   0.4s\n",
      "[CV 2/5] END learning_rate=0.01, n_estimators=94, random_state=806;, score=0.709 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.01, n_estimators=94, random_state=806;, score=0.660 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.01, n_estimators=94, random_state=806;, score=0.684 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.01, n_estimators=94, random_state=806;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=100, n_estimators=33, random_state=377;, score=0.384 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, n_estimators=33, random_state=377;, score=0.293 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, n_estimators=33, random_state=377;, score=0.342 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, n_estimators=33, random_state=377;, score=0.312 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, n_estimators=33, random_state=377;, score=0.338 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=74, random_state=853;, score=0.380 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=74, random_state=853;, score=0.295 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=74, random_state=853;, score=0.348 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=74, random_state=853;, score=0.310 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=74, random_state=853;, score=0.343 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.0001, n_estimators=54, random_state=53;, score=0.616 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.0001, n_estimators=54, random_state=53;, score=0.709 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.0001, n_estimators=54, random_state=53;, score=0.660 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.0001, n_estimators=54, random_state=53;, score=0.684 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.0001, n_estimators=54, random_state=53;, score=0.662 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.01, n_estimators=17, random_state=27;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, n_estimators=17, random_state=27;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, n_estimators=17, random_state=27;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, n_estimators=17, random_state=27;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, n_estimators=17, random_state=27;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=75, random_state=7;, score=0.607 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=75, random_state=7;, score=0.707 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=75, random_state=7;, score=0.654 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=75, random_state=7;, score=0.684 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=75, random_state=7;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=7, random_state=718;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=7, random_state=718;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=7, random_state=718;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=7, random_state=718;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=7, random_state=718;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=23, random_state=779;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=23, random_state=779;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=23, random_state=779;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=23, random_state=779;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=23, random_state=779;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=77, random_state=951;, score=0.616 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=77, random_state=951;, score=0.709 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=77, random_state=951;, score=0.660 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=77, random_state=951;, score=0.684 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=77, random_state=951;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=79, random_state=986;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=79, random_state=986;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=79, random_state=986;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=79, random_state=986;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=79, random_state=986;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=8, random_state=742;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=8, random_state=742;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=8, random_state=742;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=8, random_state=742;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=8, random_state=742;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=82, random_state=32;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=82, random_state=32;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=82, random_state=32;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=82, random_state=32;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=82, random_state=32;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=23, random_state=634;, score=0.624 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=23, random_state=634;, score=0.699 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=23, random_state=634;, score=0.641 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=23, random_state=634;, score=0.665 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=23, random_state=634;, score=0.640 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=16, random_state=757;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=16, random_state=757;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=16, random_state=757;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=16, random_state=757;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=16, random_state=757;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=1, random_state=276;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=1, random_state=276;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=1, random_state=276;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=1, random_state=276;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=1, random_state=276;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=91, random_state=132;, score=0.618 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=91, random_state=132;, score=0.690 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=91, random_state=132;, score=0.652 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=91, random_state=132;, score=0.660 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=91, random_state=132;, score=0.653 total time=   0.4s\n",
      "[CV 1/5] END learning_rate=100, n_estimators=86, random_state=644;, score=0.384 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, n_estimators=86, random_state=644;, score=0.293 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, n_estimators=86, random_state=644;, score=0.342 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, n_estimators=86, random_state=644;, score=0.312 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, n_estimators=86, random_state=644;, score=0.338 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=100, n_estimators=39, random_state=414;, score=0.384 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, n_estimators=39, random_state=414;, score=0.293 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, n_estimators=39, random_state=414;, score=0.342 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, n_estimators=39, random_state=414;, score=0.312 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, n_estimators=39, random_state=414;, score=0.338 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=45, random_state=797;, score=0.614 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=45, random_state=797;, score=0.692 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=45, random_state=797;, score=0.641 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=45, random_state=797;, score=0.645 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=45, random_state=797;, score=0.655 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.01, n_estimators=25, random_state=572;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, n_estimators=25, random_state=572;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, n_estimators=25, random_state=572;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, n_estimators=25, random_state=572;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, n_estimators=25, random_state=572;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=98, random_state=724;, score=0.620 total time=   0.4s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=98, random_state=724;, score=0.696 total time=   0.4s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=98, random_state=724;, score=0.654 total time=   0.5s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=98, random_state=724;, score=0.662 total time=   0.4s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=98, random_state=724;, score=0.655 total time=   0.4s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=99, random_state=189;, score=0.607 total time=   0.4s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=99, random_state=189;, score=0.688 total time=   0.4s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=99, random_state=189;, score=0.635 total time=   0.4s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=99, random_state=189;, score=0.641 total time=   0.4s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=99, random_state=189;, score=0.653 total time=   0.4s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=49, random_state=597;, score=0.616 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=49, random_state=597;, score=0.709 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=49, random_state=597;, score=0.660 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=49, random_state=597;, score=0.684 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=49, random_state=597;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.0001, n_estimators=48, random_state=769;, score=0.616 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.0001, n_estimators=48, random_state=769;, score=0.709 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.0001, n_estimators=48, random_state=769;, score=0.660 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.0001, n_estimators=48, random_state=769;, score=0.684 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.0001, n_estimators=48, random_state=769;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=68, random_state=706;, score=0.616 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=68, random_state=706;, score=0.686 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=68, random_state=706;, score=0.635 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=68, random_state=706;, score=0.633 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=68, random_state=706;, score=0.649 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=45, random_state=269;, score=0.614 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=45, random_state=269;, score=0.692 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=45, random_state=269;, score=0.641 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=45, random_state=269;, score=0.645 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=45, random_state=269;, score=0.655 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=83, random_state=806;, score=0.616 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=83, random_state=806;, score=0.692 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=83, random_state=806;, score=0.652 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=83, random_state=806;, score=0.658 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=83, random_state=806;, score=0.660 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=72, random_state=901;, score=0.609 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=72, random_state=901;, score=0.679 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=72, random_state=901;, score=0.635 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=72, random_state=901;, score=0.641 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=72, random_state=901;, score=0.651 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=68, random_state=729;, score=0.616 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=68, random_state=729;, score=0.709 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=68, random_state=729;, score=0.660 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=68, random_state=729;, score=0.684 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=68, random_state=729;, score=0.662 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=14, random_state=785;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=14, random_state=785;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=14, random_state=785;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=14, random_state=785;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=14, random_state=785;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=60, random_state=744;, score=0.380 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=60, random_state=744;, score=0.295 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=60, random_state=744;, score=0.348 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=60, random_state=744;, score=0.310 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=60, random_state=744;, score=0.343 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=47, random_state=571;, score=0.607 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=47, random_state=571;, score=0.707 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=47, random_state=571;, score=0.654 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=47, random_state=571;, score=0.684 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=47, random_state=571;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=72, random_state=72;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=72, random_state=72;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=72, random_state=72;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=72, random_state=72;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=72, random_state=72;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.0001, n_estimators=24, random_state=837;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.0001, n_estimators=24, random_state=837;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.0001, n_estimators=24, random_state=837;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.0001, n_estimators=24, random_state=837;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.0001, n_estimators=24, random_state=837;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=31, random_state=992;, score=0.616 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=31, random_state=992;, score=0.701 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=31, random_state=992;, score=0.645 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=31, random_state=992;, score=0.650 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=31, random_state=992;, score=0.643 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=91, random_state=711;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=91, random_state=711;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=91, random_state=711;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=91, random_state=711;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=91, random_state=711;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=95, random_state=961;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=95, random_state=961;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=95, random_state=961;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=95, random_state=961;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=95, random_state=961;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=87, random_state=962;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=87, random_state=962;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=87, random_state=962;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=87, random_state=962;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=87, random_state=962;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=48, random_state=671;, score=0.614 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=48, random_state=671;, score=0.703 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=48, random_state=671;, score=0.660 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=48, random_state=671;, score=0.677 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=48, random_state=671;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=23, random_state=955;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=23, random_state=955;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=23, random_state=955;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=23, random_state=955;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=23, random_state=955;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=87, random_state=654;, score=0.607 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=87, random_state=654;, score=0.707 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=87, random_state=654;, score=0.654 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=87, random_state=654;, score=0.684 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=87, random_state=654;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=42, random_state=690;, score=0.380 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=42, random_state=690;, score=0.295 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=42, random_state=690;, score=0.348 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=42, random_state=690;, score=0.310 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=42, random_state=690;, score=0.343 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=37, random_state=232;, score=0.611 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=37, random_state=232;, score=0.699 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=37, random_state=232;, score=0.648 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=37, random_state=232;, score=0.650 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=37, random_state=232;, score=0.651 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=40, random_state=97;, score=0.380 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=40, random_state=97;, score=0.295 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=40, random_state=97;, score=0.348 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=40, random_state=97;, score=0.310 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=40, random_state=97;, score=0.343 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=56, random_state=666;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=56, random_state=666;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=56, random_state=666;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=56, random_state=666;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=56, random_state=666;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=95, random_state=156;, score=0.620 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=95, random_state=156;, score=0.692 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=95, random_state=156;, score=0.650 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=95, random_state=156;, score=0.660 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=95, random_state=156;, score=0.655 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=95, random_state=433;, score=0.607 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=95, random_state=433;, score=0.707 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=95, random_state=433;, score=0.654 total time=   0.4s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=95, random_state=433;, score=0.684 total time=   0.4s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=95, random_state=433;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=27, random_state=768;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=27, random_state=768;, score=0.701 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=27, random_state=768;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=27, random_state=768;, score=0.679 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=27, random_state=768;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=15, random_state=445;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=15, random_state=445;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=15, random_state=445;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=15, random_state=445;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=15, random_state=445;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, n_estimators=65, random_state=744;, score=0.616 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, n_estimators=65, random_state=744;, score=0.709 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, n_estimators=65, random_state=744;, score=0.660 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, n_estimators=65, random_state=744;, score=0.684 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, n_estimators=65, random_state=744;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=20, random_state=755;, score=0.620 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=20, random_state=755;, score=0.699 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=20, random_state=755;, score=0.650 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=20, random_state=755;, score=0.669 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=20, random_state=755;, score=0.638 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=45, random_state=834;, score=0.607 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=45, random_state=834;, score=0.707 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=45, random_state=834;, score=0.654 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=45, random_state=834;, score=0.684 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=45, random_state=834;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=56, random_state=670;, score=0.611 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=56, random_state=670;, score=0.703 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=56, random_state=670;, score=0.652 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=56, random_state=670;, score=0.658 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=56, random_state=670;, score=0.664 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=51, random_state=844;, score=0.607 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=51, random_state=844;, score=0.707 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=51, random_state=844;, score=0.654 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=51, random_state=844;, score=0.684 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=51, random_state=844;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=93, random_state=477;, score=0.616 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=93, random_state=477;, score=0.709 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=93, random_state=477;, score=0.660 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=93, random_state=477;, score=0.684 total time=   0.4s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=93, random_state=477;, score=0.662 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=6, random_state=886;, score=0.389 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=6, random_state=886;, score=0.291 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=6, random_state=886;, score=0.344 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=6, random_state=886;, score=0.312 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=6, random_state=886;, score=0.343 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=86, random_state=665;, score=0.620 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=86, random_state=665;, score=0.692 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=86, random_state=665;, score=0.652 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=86, random_state=665;, score=0.660 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=86, random_state=665;, score=0.657 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=82, random_state=751;, score=0.380 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=82, random_state=751;, score=0.295 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=82, random_state=751;, score=0.348 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=82, random_state=751;, score=0.310 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=82, random_state=751;, score=0.343 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.0001, n_estimators=26, random_state=247;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.0001, n_estimators=26, random_state=247;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.0001, n_estimators=26, random_state=247;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.0001, n_estimators=26, random_state=247;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.0001, n_estimators=26, random_state=247;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=32, random_state=256;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=32, random_state=256;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=32, random_state=256;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=32, random_state=256;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=32, random_state=256;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=56, random_state=601;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=56, random_state=601;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=56, random_state=601;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=56, random_state=601;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=56, random_state=601;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=80, random_state=443;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=80, random_state=443;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=80, random_state=443;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=80, random_state=443;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=80, random_state=443;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=19, random_state=410;, score=0.607 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=19, random_state=410;, score=0.707 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=19, random_state=410;, score=0.656 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=19, random_state=410;, score=0.682 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=19, random_state=410;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, n_estimators=61, random_state=814;, score=0.616 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, n_estimators=61, random_state=814;, score=0.709 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, n_estimators=61, random_state=814;, score=0.660 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, n_estimators=61, random_state=814;, score=0.684 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, n_estimators=61, random_state=814;, score=0.662 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=100, n_estimators=40, random_state=153;, score=0.384 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, n_estimators=40, random_state=153;, score=0.293 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, n_estimators=40, random_state=153;, score=0.342 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, n_estimators=40, random_state=153;, score=0.312 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, n_estimators=40, random_state=153;, score=0.338 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=67, random_state=61;, score=0.618 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=67, random_state=61;, score=0.694 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=67, random_state=61;, score=0.652 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=67, random_state=61;, score=0.656 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=67, random_state=61;, score=0.660 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=51, random_state=890;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=51, random_state=890;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=51, random_state=890;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=51, random_state=890;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=51, random_state=890;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=24, random_state=779;, score=0.618 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=24, random_state=779;, score=0.694 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=24, random_state=779;, score=0.637 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=24, random_state=779;, score=0.665 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=24, random_state=779;, score=0.640 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=26, random_state=698;, score=0.380 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=26, random_state=698;, score=0.295 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=26, random_state=698;, score=0.348 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=26, random_state=698;, score=0.310 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=26, random_state=698;, score=0.343 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=92, random_state=454;, score=0.380 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=92, random_state=454;, score=0.295 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=92, random_state=454;, score=0.348 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=92, random_state=454;, score=0.310 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=92, random_state=454;, score=0.343 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.1, n_estimators=14, random_state=185;, score=0.616 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, n_estimators=14, random_state=185;, score=0.709 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, n_estimators=14, random_state=185;, score=0.660 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, n_estimators=14, random_state=185;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, n_estimators=14, random_state=185;, score=0.662 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, n_estimators=41, random_state=457;, score=0.607 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=10, n_estimators=41, random_state=457;, score=0.707 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=10, n_estimators=41, random_state=457;, score=0.654 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=10, n_estimators=41, random_state=457;, score=0.684 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=10, n_estimators=41, random_state=457;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=20, random_state=397;, score=0.620 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=20, random_state=397;, score=0.699 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=20, random_state=397;, score=0.650 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=20, random_state=397;, score=0.669 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=20, random_state=397;, score=0.638 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1000, n_estimators=32, random_state=555;, score=0.471 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1000, n_estimators=32, random_state=555;, score=0.473 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1000, n_estimators=32, random_state=555;, score=0.473 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1000, n_estimators=32, random_state=555;, score=0.473 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1000, n_estimators=32, random_state=555;, score=0.472 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, n_estimators=42, random_state=860;, score=0.616 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.001, n_estimators=42, random_state=860;, score=0.709 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.001, n_estimators=42, random_state=860;, score=0.660 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.001, n_estimators=42, random_state=860;, score=0.684 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.001, n_estimators=42, random_state=860;, score=0.662 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=100, n_estimators=15, random_state=107;, score=0.384 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, n_estimators=15, random_state=107;, score=0.293 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, n_estimators=15, random_state=107;, score=0.342 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, n_estimators=15, random_state=107;, score=0.312 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, n_estimators=15, random_state=107;, score=0.338 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, n_estimators=41, random_state=355;, score=0.607 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=1, n_estimators=41, random_state=355;, score=0.690 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=1, n_estimators=41, random_state=355;, score=0.639 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=1, n_estimators=41, random_state=355;, score=0.650 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=1, n_estimators=41, random_state=355;, score=0.647 total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=AdaBoostClassifier(), n_iter=100,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        &#x27;n_estimators&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 6...\n",
       "       911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923,\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999])},\n",
       "                   verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=AdaBoostClassifier(), n_iter=100,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        &#x27;n_estimators&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 6...\n",
       "       911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923,\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999])},\n",
       "                   verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=AdaBoostClassifier(), n_iter=100,\n",
       "                   param_distributions={'learning_rate': [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        'n_estimators': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 6...\n",
       "       911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923,\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999])},\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se fitea el modelo\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros: {'random_state': 545, 'n_estimators': 1, 'learning_rate': 100}\n",
      "Mejor Score: 0.6660983873153543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Mejores parametros: \"+str(model.best_params_))\n",
    "print(\"Mejor Score: \"+str(model.best_score_)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_random_state</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.108703</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>779</td>\n",
       "      <td>23</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'random_state': 779, 'n_estimators': 23, 'learning_rate': 0.001}</td>\n",
       "      <td>0.615711</td>\n",
       "      <td>0.70913</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.683652</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.666098</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.271415</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>369</td>\n",
       "      <td>59</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'random_state': 369, 'n_estimators': 59, 'learning_rate': 0.0001}</td>\n",
       "      <td>0.615711</td>\n",
       "      <td>0.70913</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.683652</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.666098</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.411918</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>477</td>\n",
       "      <td>93</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'random_state': 477, 'n_estimators': 93, 'learning_rate': 0.001}</td>\n",
       "      <td>0.615711</td>\n",
       "      <td>0.70913</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.683652</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.666098</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.077649</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>757</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'random_state': 757, 'n_estimators': 16, 'learning_rate': 0.001}</td>\n",
       "      <td>0.615711</td>\n",
       "      <td>0.70913</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.683652</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.666098</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.077968</td>\n",
       "      <td>0.010019</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'random_state': 27, 'n_estimators': 17, 'learning_rate': 0.01}</td>\n",
       "      <td>0.615711</td>\n",
       "      <td>0.70913</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.683652</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.666098</td>\n",
       "      <td>0.030827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "30       0.108703      0.003790         0.001604        0.003207   \n",
       "15       0.271415      0.014037         0.005781        0.006071   \n",
       "77       0.411918      0.019158         0.020867        0.005847   \n",
       "36       0.077649      0.005192         0.003599        0.003001   \n",
       "27       0.077968      0.010019         0.001406        0.002813   \n",
       "\n",
       "   param_random_state param_n_estimators param_learning_rate  \\\n",
       "30                779                 23               0.001   \n",
       "15                369                 59              0.0001   \n",
       "77                477                 93               0.001   \n",
       "36                757                 16               0.001   \n",
       "27                 27                 17                0.01   \n",
       "\n",
       "                                                                params  \\\n",
       "30   {'random_state': 779, 'n_estimators': 23, 'learning_rate': 0.001}   \n",
       "15  {'random_state': 369, 'n_estimators': 59, 'learning_rate': 0.0001}   \n",
       "77   {'random_state': 477, 'n_estimators': 93, 'learning_rate': 0.001}   \n",
       "36   {'random_state': 757, 'n_estimators': 16, 'learning_rate': 0.001}   \n",
       "27     {'random_state': 27, 'n_estimators': 17, 'learning_rate': 0.01}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "30           0.615711            0.70913           0.660297   \n",
       "15           0.615711            0.70913           0.660297   \n",
       "77           0.615711            0.70913           0.660297   \n",
       "36           0.615711            0.70913           0.660297   \n",
       "27           0.615711            0.70913           0.660297   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "30           0.683652           0.661702         0.666098        0.030827   \n",
       "15           0.683652           0.661702         0.666098        0.030827   \n",
       "77           0.683652           0.661702         0.666098        0.030827   \n",
       "36           0.683652           0.661702         0.666098        0.030827   \n",
       "27           0.683652           0.661702         0.666098        0.030827   \n",
       "\n",
       "    rank_test_score  \n",
       "30                1  \n",
       "15                1  \n",
       "77                1  \n",
       "36                1  \n",
       "27                1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veamos los resultados obtenidos\n",
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scores.sort_values(\"mean_test_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ejecuta la predicción de resultados con X_test\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6621392190152802\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[189  87]\n",
      " [112 201]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier is: 0.6621392190152802\n",
      "Precision Score of the classifier is: 0.6979166666666666\n",
      "Recall Score of the classifier is: 0.6421725239616614\n",
      "F1 Score of the classifier is: 0.6688851913477537\n",
      "AUC for our classifier is: 0.6634775663286567\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(f\"Accuracy of the classifier is: {accuracy_score(y_test, predictions)}\")\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "print(f\"Precision Score of the classifier is: {precision_score(y_test, predictions)}\")\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(f\"Recall Score of the classifier is: {recall_score(y_test, predictions)}\")\n",
    "\n",
    "# Calculo del F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 Score of the classifier is: {f1_score(y_test, predictions)}\")\n",
    "\n",
    "#Template CURVA - ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "class_probabilities = model.predict_proba(X_test)\n",
    "preds = class_probabilities[:, 1]\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# AUC\n",
    "print(f\"AUC for our classifier is: {roc_auc}\")\n",
    "\n",
    "# Gráfica de la Curva ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Al modelo le cuesta detectar las clases (hay mucho falsos positivos y negativos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen X e y\n",
    "X = BTC_metals_2.drop (['Target'],axis=1)\n",
    "y = BTC_metals_2 ['Target']\n",
    "\n",
    "# Dividimos los datos en Train y Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el modelo Gradient Boosting Classifier\n",
    "gbrt = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye la grilla de RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definelos hyperparámetros\n",
    "param_grid = {'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'n_estimators':[100, 1000, 10000],\n",
    "              'subsample':[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.99],\n",
    "              'criterion': ['squared_error'],\n",
    "              'warm_start': [False, True],\n",
    "              'random_state': np.arange (1,1000,1)\n",
    "              }\n",
    "\n",
    "# Se uitiliza la RandomizedSearchCV y se agrega Cross validation con k=5 \n",
    "model = RandomizedSearchCV(gbrt, param_grid, cv=5, n_iter=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=830, subsample=0.0001, warm_start=False; total time=   0.7s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=830, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=830, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=830, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=830, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=416, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=416, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=416, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=416, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=416, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=100, random_state=762, subsample=0.1, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=100, random_state=762, subsample=0.1, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=100, random_state=762, subsample=0.1, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=100, random_state=762, subsample=0.1, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=100, random_state=762, subsample=0.1, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=986, subsample=1e-05, warm_start=False; total time=   7.7s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=986, subsample=1e-05, warm_start=False; total time=   6.9s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=986, subsample=1e-05, warm_start=False; total time=   6.9s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=986, subsample=1e-05, warm_start=False; total time=   6.9s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=986, subsample=1e-05, warm_start=False; total time=   7.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=959, subsample=0.0001, warm_start=False; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=959, subsample=0.0001, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=959, subsample=0.0001, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=959, subsample=0.0001, warm_start=False; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=959, subsample=0.0001, warm_start=False; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=671, subsample=0.1, warm_start=False; total time=   0.1s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=671, subsample=0.1, warm_start=False; total time=   0.1s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=671, subsample=0.1, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=671, subsample=0.1, warm_start=False; total time=   0.1s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=671, subsample=0.1, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=682, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=682, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=682, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=682, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=682, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=317, subsample=0.001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=317, subsample=0.001, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=317, subsample=0.001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=317, subsample=0.001, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=317, subsample=0.001, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=801, subsample=0.0001, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=801, subsample=0.0001, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=801, subsample=0.0001, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=801, subsample=0.0001, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=801, subsample=0.0001, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=787, subsample=0.0001, warm_start=False; total time=   5.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=787, subsample=0.0001, warm_start=False; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=787, subsample=0.0001, warm_start=False; total time=   5.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=787, subsample=0.0001, warm_start=False; total time=   5.7s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=787, subsample=0.0001, warm_start=False; total time=   5.4s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=737, subsample=0.01, warm_start=False; total time=   9.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=737, subsample=0.01, warm_start=False; total time=   8.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=737, subsample=0.01, warm_start=False; total time=   9.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=737, subsample=0.01, warm_start=False; total time=   8.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=737, subsample=0.01, warm_start=False; total time=   8.8s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=505, subsample=0.001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=505, subsample=0.001, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=505, subsample=0.001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=505, subsample=0.001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=505, subsample=0.001, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=10, subsample=0.001, warm_start=True; total time=   6.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=10, subsample=0.001, warm_start=True; total time=   7.3s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=10, subsample=0.001, warm_start=True; total time=   6.7s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=10, subsample=0.001, warm_start=True; total time=   6.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=10, subsample=0.001, warm_start=True; total time=   6.6s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=647, subsample=0.99, warm_start=False; total time=   6.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=647, subsample=0.99, warm_start=False; total time=   6.1s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=647, subsample=0.99, warm_start=False; total time=   6.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=647, subsample=0.99, warm_start=False; total time=   6.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=647, subsample=0.99, warm_start=False; total time=   6.2s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=847, subsample=0.99, warm_start=True; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=847, subsample=0.99, warm_start=True; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=847, subsample=0.99, warm_start=True; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=847, subsample=0.99, warm_start=True; total time= 1.0min\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=847, subsample=0.99, warm_start=True; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=205, subsample=0.001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=205, subsample=0.001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=205, subsample=0.001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=205, subsample=0.001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=205, subsample=0.001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=675, subsample=0.0001, warm_start=False; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=675, subsample=0.0001, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=675, subsample=0.0001, warm_start=False; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=675, subsample=0.0001, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=675, subsample=0.0001, warm_start=False; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=3, subsample=0.01, warm_start=False; total time=   0.7s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=3, subsample=0.01, warm_start=False; total time=   0.7s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=3, subsample=0.01, warm_start=False; total time=   0.7s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=3, subsample=0.01, warm_start=False; total time=   0.7s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=3, subsample=0.01, warm_start=False; total time=   0.7s\n",
      "[CV] END criterion=squared_error, learning_rate=10, n_estimators=1000, random_state=381, subsample=0.1, warm_start=True; total time=   1.3s\n",
      "[CV] END criterion=squared_error, learning_rate=10, n_estimators=1000, random_state=381, subsample=0.1, warm_start=True; total time=   1.3s\n",
      "[CV] END criterion=squared_error, learning_rate=10, n_estimators=1000, random_state=381, subsample=0.1, warm_start=True; total time=   1.3s\n",
      "[CV] END criterion=squared_error, learning_rate=10, n_estimators=1000, random_state=381, subsample=0.1, warm_start=True; total time=   1.3s\n",
      "[CV] END criterion=squared_error, learning_rate=10, n_estimators=1000, random_state=381, subsample=0.1, warm_start=True; total time=   1.3s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=357, subsample=0.99, warm_start=True; total time=   6.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=357, subsample=0.99, warm_start=True; total time=   6.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=357, subsample=0.99, warm_start=True; total time=   6.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=357, subsample=0.99, warm_start=True; total time=   6.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=357, subsample=0.99, warm_start=True; total time=   6.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=637, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=637, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=637, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=637, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=637, subsample=0.0001, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=311, subsample=1e-05, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=311, subsample=1e-05, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=311, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=311, subsample=1e-05, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=311, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=758, subsample=0.99, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=758, subsample=0.99, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=758, subsample=0.99, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=758, subsample=0.99, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=100, random_state=758, subsample=0.99, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=303, subsample=0.01, warm_start=True; total time=   7.9s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=303, subsample=0.01, warm_start=True; total time=   8.1s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=303, subsample=0.01, warm_start=True; total time=   8.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=303, subsample=0.01, warm_start=True; total time=   8.7s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=303, subsample=0.01, warm_start=True; total time=   7.9s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=937, subsample=0.99, warm_start=False; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=937, subsample=0.99, warm_start=False; total time= 1.0min\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=937, subsample=0.99, warm_start=False; total time= 1.0min\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=937, subsample=0.99, warm_start=False; total time= 1.1min\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=937, subsample=0.99, warm_start=False; total time= 1.0min\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=490, subsample=1e-05, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=490, subsample=1e-05, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=490, subsample=1e-05, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=490, subsample=1e-05, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=490, subsample=1e-05, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=339, subsample=0.1, warm_start=True; total time=  14.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=339, subsample=0.1, warm_start=True; total time=  14.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=339, subsample=0.1, warm_start=True; total time=  14.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=339, subsample=0.1, warm_start=True; total time=  14.1s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=339, subsample=0.1, warm_start=True; total time=  14.9s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=130, subsample=0.99, warm_start=False; total time=  13.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=130, subsample=0.99, warm_start=False; total time=  13.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=130, subsample=0.99, warm_start=False; total time=  13.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=130, subsample=0.99, warm_start=False; total time=  13.3s\n",
      "[CV] END criterion=squared_error, learning_rate=1, n_estimators=10000, random_state=130, subsample=0.99, warm_start=False; total time=  13.6s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=382, subsample=0.1, warm_start=True; total time=  14.0s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=382, subsample=0.1, warm_start=True; total time=  14.2s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=382, subsample=0.1, warm_start=True; total time=  14.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=382, subsample=0.1, warm_start=True; total time=  14.0s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=382, subsample=0.1, warm_start=True; total time=  14.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=617, subsample=1e-05, warm_start=True; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=617, subsample=1e-05, warm_start=True; total time=   5.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=617, subsample=1e-05, warm_start=True; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=617, subsample=1e-05, warm_start=True; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=617, subsample=1e-05, warm_start=True; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=792, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=792, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=792, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=792, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=1000, random_state=792, subsample=1e-05, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=142, subsample=0.0001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=142, subsample=0.0001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=142, subsample=0.0001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=142, subsample=0.0001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=142, subsample=0.0001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=496, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=496, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=496, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=496, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=1000, random_state=496, subsample=0.1, warm_start=False; total time=   1.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=100, random_state=816, subsample=0.001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=100, random_state=816, subsample=0.001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=100, random_state=816, subsample=0.001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=100, random_state=816, subsample=0.001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=100, random_state=816, subsample=0.001, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=289, subsample=0.01, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=289, subsample=0.01, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=289, subsample=0.01, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=289, subsample=0.01, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=289, subsample=0.01, warm_start=False; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=557, subsample=0.0001, warm_start=False; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=557, subsample=0.0001, warm_start=False; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=557, subsample=0.0001, warm_start=False; total time=   5.8s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=557, subsample=0.0001, warm_start=False; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=10000, random_state=557, subsample=0.0001, warm_start=False; total time=   5.2s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=555, subsample=0.1, warm_start=True; total time=  13.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=555, subsample=0.1, warm_start=True; total time=  14.1s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=555, subsample=0.1, warm_start=True; total time=  14.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=555, subsample=0.1, warm_start=True; total time=  14.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=555, subsample=0.1, warm_start=True; total time=  13.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=791, subsample=0.01, warm_start=False; total time=   0.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=791, subsample=0.01, warm_start=False; total time=   0.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=791, subsample=0.01, warm_start=False; total time=   0.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=791, subsample=0.01, warm_start=False; total time=   0.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=1000, random_state=791, subsample=0.01, warm_start=False; total time=   0.8s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=97, subsample=0.0001, warm_start=True; total time=   7.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=97, subsample=0.0001, warm_start=True; total time=   7.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=97, subsample=0.0001, warm_start=True; total time=   7.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=97, subsample=0.0001, warm_start=True; total time=   7.1s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=97, subsample=0.0001, warm_start=True; total time=   6.6s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=801, subsample=0.0001, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=801, subsample=0.0001, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=801, subsample=0.0001, warm_start=False; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=801, subsample=0.0001, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.001, n_estimators=1000, random_state=801, subsample=0.0001, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=740, subsample=0.0001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=740, subsample=0.0001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=740, subsample=0.0001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=740, subsample=0.0001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=1000, random_state=740, subsample=0.0001, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=992, subsample=0.0001, warm_start=True; total time=   6.0s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=992, subsample=0.0001, warm_start=True; total time=   5.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=992, subsample=0.0001, warm_start=True; total time=   5.2s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=992, subsample=0.0001, warm_start=True; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.01, n_estimators=10000, random_state=992, subsample=0.0001, warm_start=True; total time=   5.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=989, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=989, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=989, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=989, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.0001, n_estimators=100, random_state=989, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=154, subsample=1e-05, warm_start=False; total time=   5.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=154, subsample=1e-05, warm_start=False; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=154, subsample=1e-05, warm_start=False; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=154, subsample=1e-05, warm_start=False; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=10000, random_state=154, subsample=1e-05, warm_start=False; total time=   5.3s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=40, subsample=0.1, warm_start=True; total time=  14.1s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=40, subsample=0.1, warm_start=True; total time=  13.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=40, subsample=0.1, warm_start=True; total time=  14.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=40, subsample=0.1, warm_start=True; total time=  14.3s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=10000, random_state=40, subsample=0.1, warm_start=True; total time=  14.0s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=122, subsample=0.0001, warm_start=False; total time=   5.6s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=122, subsample=0.0001, warm_start=False; total time=   5.6s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=122, subsample=0.0001, warm_start=False; total time=   5.2s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=122, subsample=0.0001, warm_start=False; total time=   5.2s\n",
      "[CV] END criterion=squared_error, learning_rate=100, n_estimators=10000, random_state=122, subsample=0.0001, warm_start=False; total time=   5.6s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=531, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=531, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=531, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=531, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=100, random_state=531, subsample=0.99, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=625, subsample=1e-05, warm_start=True; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=625, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=625, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=625, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=0.1, n_estimators=1000, random_state=625, subsample=1e-05, warm_start=True; total time=   0.4s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=795, subsample=0.1, warm_start=True; total time=   0.1s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=795, subsample=0.1, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=795, subsample=0.1, warm_start=True; total time=   0.1s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=795, subsample=0.1, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=795, subsample=0.1, warm_start=True; total time=   0.0s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=36, subsample=0.99, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=36, subsample=0.99, warm_start=False; total time=   0.6s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=36, subsample=0.99, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=36, subsample=0.99, warm_start=False; total time=   0.5s\n",
      "[CV] END criterion=squared_error, learning_rate=1000, n_estimators=100, random_state=36, subsample=0.99, warm_start=False; total time=   0.5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_iter=50,\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;squared_error&#x27;],\n",
       "                                        &#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 1000, 10000],\n",
       "                                        &#x27;random_state&#x27;: array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  4...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        &#x27;subsample&#x27;: [1e-05, 0.0001, 0.001,\n",
       "                                                      0.01, 0.1, 0.99],\n",
       "                                        &#x27;warm_start&#x27;: [False, True]},\n",
       "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_iter=50,\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;squared_error&#x27;],\n",
       "                                        &#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 1000, 10000],\n",
       "                                        &#x27;random_state&#x27;: array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  4...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        &#x27;subsample&#x27;: [1e-05, 0.0001, 0.001,\n",
       "                                                      0.01, 0.1, 0.99],\n",
       "                                        &#x27;warm_start&#x27;: [False, True]},\n",
       "                   verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=GradientBoostingClassifier(), n_iter=50,\n",
       "                   param_distributions={'criterion': ['squared_error'],\n",
       "                                        'learning_rate': [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        'n_estimators': [100, 1000, 10000],\n",
       "                                        'random_state': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  4...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        'subsample': [1e-05, 0.0001, 0.001,\n",
       "                                                      0.01, 0.1, 0.99],\n",
       "                                        'warm_start': [False, True]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros: {'warm_start': True, 'subsample': 0.01, 'random_state': 303, 'n_estimators': 10000, 'learning_rate': 0.0001, 'criterion': 'squared_error'}\n",
      "Mejor Score: 0.6723300970873787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Mejores parametros: \"+str(model.best_params_))\n",
    "print(\"Mejor Score: \"+str(model.best_score_)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_warm_start</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_random_state</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.162959</td>\n",
       "      <td>0.278752</td>\n",
       "      <td>0.085048</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>303</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>{'warm_start': True, 'subsample': 0.01, 'random_state': 303, 'n_estimators': 10000, 'learning_rate': 0.0001, 'criterion': 'squared_error'}</td>\n",
       "      <td>0.686893</td>\n",
       "      <td>0.672330</td>\n",
       "      <td>0.672330</td>\n",
       "      <td>0.672330</td>\n",
       "      <td>0.657767</td>\n",
       "      <td>0.672330</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.806995</td>\n",
       "      <td>0.007509</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>0.007656</td>\n",
       "      <td>False</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>{'warm_start': False, 'subsample': 0.01, 'random_state': 3, 'n_estimators': 1000, 'learning_rate': 0.001, 'criterion': 'squared_error'}</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>0.667476</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>0.652913</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.630091</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.99</td>\n",
       "      <td>531</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>{'warm_start': True, 'subsample': 0.99, 'random_state': 531, 'n_estimators': 100, 'learning_rate': 0.1, 'criterion': 'squared_error'}</td>\n",
       "      <td>0.677184</td>\n",
       "      <td>0.628641</td>\n",
       "      <td>0.650485</td>\n",
       "      <td>0.645631</td>\n",
       "      <td>0.633495</td>\n",
       "      <td>0.647087</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.145203</td>\n",
       "      <td>0.005765</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>795</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>{'warm_start': True, 'subsample': 0.1, 'random_state': 795, 'n_estimators': 100, 'learning_rate': 1000, 'criterion': 'squared_error'}</td>\n",
       "      <td>0.667476</td>\n",
       "      <td>0.611650</td>\n",
       "      <td>0.643204</td>\n",
       "      <td>0.648058</td>\n",
       "      <td>0.626214</td>\n",
       "      <td>0.639320</td>\n",
       "      <td>0.019087</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>63.046368</td>\n",
       "      <td>0.210278</td>\n",
       "      <td>0.059345</td>\n",
       "      <td>0.011670</td>\n",
       "      <td>False</td>\n",
       "      <td>0.99</td>\n",
       "      <td>937</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>squared_error</td>\n",
       "      <td>{'warm_start': False, 'subsample': 0.99, 'random_state': 937, 'n_estimators': 10000, 'learning_rate': 0.01, 'criterion': 'squared_error'}</td>\n",
       "      <td>0.662621</td>\n",
       "      <td>0.618932</td>\n",
       "      <td>0.631068</td>\n",
       "      <td>0.665049</td>\n",
       "      <td>0.604369</td>\n",
       "      <td>0.636408</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "23       8.162959      0.278752         0.085048        0.007200   \n",
       "17       0.806995      0.007509         0.009376        0.007656   \n",
       "46       0.630091      0.005292         0.000000        0.000000   \n",
       "48       0.145203      0.005765         0.003127        0.006253   \n",
       "24      63.046368      0.210278         0.059345        0.011670   \n",
       "\n",
       "   param_warm_start param_subsample param_random_state param_n_estimators  \\\n",
       "23             True            0.01                303              10000   \n",
       "17            False            0.01                  3               1000   \n",
       "46             True            0.99                531                100   \n",
       "48             True             0.1                795                100   \n",
       "24            False            0.99                937              10000   \n",
       "\n",
       "   param_learning_rate param_criterion  \\\n",
       "23              0.0001   squared_error   \n",
       "17               0.001   squared_error   \n",
       "46                 0.1   squared_error   \n",
       "48                1000   squared_error   \n",
       "24                0.01   squared_error   \n",
       "\n",
       "                                                                                                                                        params  \\\n",
       "23  {'warm_start': True, 'subsample': 0.01, 'random_state': 303, 'n_estimators': 10000, 'learning_rate': 0.0001, 'criterion': 'squared_error'}   \n",
       "17     {'warm_start': False, 'subsample': 0.01, 'random_state': 3, 'n_estimators': 1000, 'learning_rate': 0.001, 'criterion': 'squared_error'}   \n",
       "46       {'warm_start': True, 'subsample': 0.99, 'random_state': 531, 'n_estimators': 100, 'learning_rate': 0.1, 'criterion': 'squared_error'}   \n",
       "48       {'warm_start': True, 'subsample': 0.1, 'random_state': 795, 'n_estimators': 100, 'learning_rate': 1000, 'criterion': 'squared_error'}   \n",
       "24   {'warm_start': False, 'subsample': 0.99, 'random_state': 937, 'n_estimators': 10000, 'learning_rate': 0.01, 'criterion': 'squared_error'}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "23           0.686893           0.672330           0.672330   \n",
       "17           0.689320           0.669903           0.667476   \n",
       "46           0.677184           0.628641           0.650485   \n",
       "48           0.667476           0.611650           0.643204   \n",
       "24           0.662621           0.618932           0.631068   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "23           0.672330           0.657767         0.672330        0.009211   \n",
       "17           0.669903           0.652913         0.669903        0.011590   \n",
       "46           0.645631           0.633495         0.647087        0.016997   \n",
       "48           0.648058           0.626214         0.639320        0.019087   \n",
       "24           0.665049           0.604369         0.636408        0.023949   \n",
       "\n",
       "    rank_test_score  \n",
       "23                1  \n",
       "17                2  \n",
       "46                3  \n",
       "48                4  \n",
       "24                5  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veamos los resultados obtenidos\n",
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scores.sort_values(\"mean_test_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ejecuta la predicción de resultados con X_test\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.636466591166478\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[240 183]\n",
      " [138 322]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier is: 0.636466591166478\n",
      "Precision Score of the classifier is: 0.6376237623762376\n",
      "Recall Score of the classifier is: 0.7\n",
      "F1 Score of the classifier is: 0.6673575129533679\n",
      "AUC for our classifier is: 0.7010124370438894\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(f\"Accuracy of the classifier is: {accuracy_score(y_test, predictions)}\")\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "print(f\"Precision Score of the classifier is: {precision_score(y_test, predictions)}\")\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(f\"Recall Score of the classifier is: {recall_score(y_test, predictions)}\")\n",
    "\n",
    "# Calculo del F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 Score of the classifier is: {f1_score(y_test, predictions)}\")\n",
    "\n",
    "#Template CURVA - ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "class_probabilities = model.predict_proba(X_test)\n",
    "preds = class_probabilities[:, 1]\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# AUC\n",
    "print(f\"AUC for our classifier is: {roc_auc}\")\n",
    "\n",
    "# Gráfica de la Curva ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Al modelo le cuesta detectar las clases (hay mucho falsos positivos y negativos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen X e y\n",
    "X = BTC_metals_2.drop (['Target'],axis=1)\n",
    "y = BTC_metals_2 ['Target']\n",
    "\n",
    "# Dividimos los datos en Train y Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye la grilla de RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definelos hyperparámetros\n",
    "param_grid = {'booster':['gbtree', 'gblinear','dart', 'gbtree and dart'],\n",
    "              'validate_parameters':[True, False],\n",
    "              'seed': np.arange (1,1000,1),\n",
    "              'n_estimators': np.arange (1,1000,1),\n",
    "              'eval_metric': ['logloss'],\n",
    "              'scale_pos_weight': [2,3]\n",
    "              }\n",
    "\n",
    "# Se uitiliza la RandomizedSearchCV y se agrega Cross validation con k=5 \n",
    "model = RandomizedSearchCV(clf_xgb, param_grid, cv=5, n_iter=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=133, scale_pos_weight=2, seed=750, validate_parameters=True; total time=   0.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=133, scale_pos_weight=2, seed=750, validate_parameters=True; total time=   0.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=133, scale_pos_weight=2, seed=750, validate_parameters=True; total time=   0.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=133, scale_pos_weight=2, seed=750, validate_parameters=True; total time=   0.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=133, scale_pos_weight=2, seed=750, validate_parameters=True; total time=   0.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=834, scale_pos_weight=3, seed=121, validate_parameters=False; total time=   2.7s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=834, scale_pos_weight=3, seed=121, validate_parameters=False; total time=   2.6s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=834, scale_pos_weight=3, seed=121, validate_parameters=False; total time=   2.7s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=834, scale_pos_weight=3, seed=121, validate_parameters=False; total time=   2.6s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=834, scale_pos_weight=3, seed=121, validate_parameters=False; total time=   2.7s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=330, scale_pos_weight=2, seed=962, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=330, scale_pos_weight=2, seed=962, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=330, scale_pos_weight=2, seed=962, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=330, scale_pos_weight=2, seed=962, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=330, scale_pos_weight=2, seed=962, validate_parameters=False; total time=   0.0s\n",
      "[16:08:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=183, scale_pos_weight=2, seed=657, validate_parameters=False; total time=   0.0s\n",
      "[16:08:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=183, scale_pos_weight=2, seed=657, validate_parameters=False; total time=   0.0s\n",
      "[16:08:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=183, scale_pos_weight=2, seed=657, validate_parameters=False; total time=   0.0s\n",
      "[16:08:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=183, scale_pos_weight=2, seed=657, validate_parameters=False; total time=   0.0s\n",
      "[16:08:18] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=183, scale_pos_weight=2, seed=657, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=26, scale_pos_weight=2, seed=438, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=26, scale_pos_weight=2, seed=438, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=26, scale_pos_weight=2, seed=438, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=26, scale_pos_weight=2, seed=438, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=26, scale_pos_weight=2, seed=438, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=209, scale_pos_weight=3, seed=630, validate_parameters=False; total time=   3.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=209, scale_pos_weight=3, seed=630, validate_parameters=False; total time=   3.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=209, scale_pos_weight=3, seed=630, validate_parameters=False; total time=   3.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=209, scale_pos_weight=3, seed=630, validate_parameters=False; total time=   3.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=209, scale_pos_weight=3, seed=630, validate_parameters=False; total time=   3.5s\n",
      "[16:08:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=599, scale_pos_weight=3, seed=31, validate_parameters=True; total time=   0.1s\n",
      "[16:08:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=599, scale_pos_weight=3, seed=31, validate_parameters=True; total time=   0.1s\n",
      "[16:08:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=599, scale_pos_weight=3, seed=31, validate_parameters=True; total time=   0.1s\n",
      "[16:08:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=599, scale_pos_weight=3, seed=31, validate_parameters=True; total time=   0.1s\n",
      "[16:08:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=599, scale_pos_weight=3, seed=31, validate_parameters=True; total time=   0.1s\n",
      "[16:08:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=240, scale_pos_weight=2, seed=278, validate_parameters=True; total time=   0.0s\n",
      "[16:08:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=240, scale_pos_weight=2, seed=278, validate_parameters=True; total time=   0.0s\n",
      "[16:08:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=240, scale_pos_weight=2, seed=278, validate_parameters=True; total time=   0.0s\n",
      "[16:08:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=240, scale_pos_weight=2, seed=278, validate_parameters=True; total time=   0.0s\n",
      "[16:08:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=240, scale_pos_weight=2, seed=278, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=487, scale_pos_weight=3, seed=905, validate_parameters=True; total time=  16.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=487, scale_pos_weight=3, seed=905, validate_parameters=True; total time=  17.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=487, scale_pos_weight=3, seed=905, validate_parameters=True; total time=  16.9s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=487, scale_pos_weight=3, seed=905, validate_parameters=True; total time=  17.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=487, scale_pos_weight=3, seed=905, validate_parameters=True; total time=  17.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=33, scale_pos_weight=3, seed=195, validate_parameters=True; total time=   0.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=33, scale_pos_weight=3, seed=195, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=33, scale_pos_weight=3, seed=195, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=33, scale_pos_weight=3, seed=195, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=33, scale_pos_weight=3, seed=195, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=787, scale_pos_weight=3, seed=173, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=787, scale_pos_weight=3, seed=173, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=787, scale_pos_weight=3, seed=173, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=787, scale_pos_weight=3, seed=173, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=787, scale_pos_weight=3, seed=173, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=45, scale_pos_weight=3, seed=914, validate_parameters=False; total time=   0.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=45, scale_pos_weight=3, seed=914, validate_parameters=False; total time=   0.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=45, scale_pos_weight=3, seed=914, validate_parameters=False; total time=   0.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=45, scale_pos_weight=3, seed=914, validate_parameters=False; total time=   0.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=45, scale_pos_weight=3, seed=914, validate_parameters=False; total time=   0.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=68, scale_pos_weight=2, seed=669, validate_parameters=False; total time=   0.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=68, scale_pos_weight=2, seed=669, validate_parameters=False; total time=   0.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=68, scale_pos_weight=2, seed=669, validate_parameters=False; total time=   0.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=68, scale_pos_weight=2, seed=669, validate_parameters=False; total time=   0.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=68, scale_pos_weight=2, seed=669, validate_parameters=False; total time=   0.5s\n",
      "[16:10:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=798, scale_pos_weight=3, seed=343, validate_parameters=False; total time=   0.2s\n",
      "[16:10:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=798, scale_pos_weight=3, seed=343, validate_parameters=False; total time=   0.2s\n",
      "[16:10:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=798, scale_pos_weight=3, seed=343, validate_parameters=False; total time=   0.2s\n",
      "[16:10:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=798, scale_pos_weight=3, seed=343, validate_parameters=False; total time=   0.2s\n",
      "[16:10:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=798, scale_pos_weight=3, seed=343, validate_parameters=False; total time=   0.2s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=3, seed=710, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=3, seed=710, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=3, seed=710, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=3, seed=710, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=3, seed=710, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=589, scale_pos_weight=2, seed=974, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=589, scale_pos_weight=2, seed=974, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=589, scale_pos_weight=2, seed=974, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=589, scale_pos_weight=2, seed=974, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=589, scale_pos_weight=2, seed=974, validate_parameters=False; total time=   0.0s\n",
      "[16:10:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=984, scale_pos_weight=2, seed=947, validate_parameters=True; total time=   0.2s\n",
      "[16:10:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=984, scale_pos_weight=2, seed=947, validate_parameters=True; total time=   0.2s\n",
      "[16:10:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=984, scale_pos_weight=2, seed=947, validate_parameters=True; total time=   0.2s\n",
      "[16:10:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=984, scale_pos_weight=2, seed=947, validate_parameters=True; total time=   0.2s\n",
      "[16:10:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=984, scale_pos_weight=2, seed=947, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=265, scale_pos_weight=2, seed=181, validate_parameters=False; total time=   5.4s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=265, scale_pos_weight=2, seed=181, validate_parameters=False; total time=   5.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=265, scale_pos_weight=2, seed=181, validate_parameters=False; total time=   5.4s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=265, scale_pos_weight=2, seed=181, validate_parameters=False; total time=   5.4s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=265, scale_pos_weight=2, seed=181, validate_parameters=False; total time=   5.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=999, scale_pos_weight=3, seed=30, validate_parameters=False; total time=   3.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=999, scale_pos_weight=3, seed=30, validate_parameters=False; total time=   3.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=999, scale_pos_weight=3, seed=30, validate_parameters=False; total time=   3.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=999, scale_pos_weight=3, seed=30, validate_parameters=False; total time=   3.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=999, scale_pos_weight=3, seed=30, validate_parameters=False; total time=   3.2s\n",
      "[16:10:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=491, scale_pos_weight=3, seed=475, validate_parameters=False; total time=   0.1s\n",
      "[16:10:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=491, scale_pos_weight=3, seed=475, validate_parameters=False; total time=   0.1s\n",
      "[16:10:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=491, scale_pos_weight=3, seed=475, validate_parameters=False; total time=   0.1s\n",
      "[16:10:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=491, scale_pos_weight=3, seed=475, validate_parameters=False; total time=   0.1s\n",
      "[16:10:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=491, scale_pos_weight=3, seed=475, validate_parameters=False; total time=   0.1s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=584, scale_pos_weight=3, seed=855, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=584, scale_pos_weight=3, seed=855, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=584, scale_pos_weight=3, seed=855, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=584, scale_pos_weight=3, seed=855, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=584, scale_pos_weight=3, seed=855, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=863, scale_pos_weight=2, seed=198, validate_parameters=False; total time=  51.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=863, scale_pos_weight=2, seed=198, validate_parameters=False; total time=  57.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=863, scale_pos_weight=2, seed=198, validate_parameters=False; total time=  50.9s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=863, scale_pos_weight=2, seed=198, validate_parameters=False; total time=  52.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=863, scale_pos_weight=2, seed=198, validate_parameters=False; total time=  49.6s\n",
      "[16:15:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=12, scale_pos_weight=3, seed=553, validate_parameters=False; total time=   0.0s\n",
      "[16:15:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=12, scale_pos_weight=3, seed=553, validate_parameters=False; total time=   0.0s\n",
      "[16:15:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=12, scale_pos_weight=3, seed=553, validate_parameters=False; total time=   0.0s\n",
      "[16:15:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=12, scale_pos_weight=3, seed=553, validate_parameters=False; total time=   0.0s\n",
      "[16:15:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=12, scale_pos_weight=3, seed=553, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=848, scale_pos_weight=3, seed=302, validate_parameters=False; total time=   2.8s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=848, scale_pos_weight=3, seed=302, validate_parameters=False; total time=   2.8s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=848, scale_pos_weight=3, seed=302, validate_parameters=False; total time=   2.8s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=848, scale_pos_weight=3, seed=302, validate_parameters=False; total time=   2.9s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=848, scale_pos_weight=3, seed=302, validate_parameters=False; total time=   3.1s\n",
      "[16:15:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=636, scale_pos_weight=3, seed=471, validate_parameters=False; total time=   0.1s\n",
      "[16:15:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=636, scale_pos_weight=3, seed=471, validate_parameters=False; total time=   0.1s\n",
      "[16:15:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=636, scale_pos_weight=3, seed=471, validate_parameters=False; total time=   0.2s\n",
      "[16:15:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=636, scale_pos_weight=3, seed=471, validate_parameters=False; total time=   0.1s\n",
      "[16:15:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=636, scale_pos_weight=3, seed=471, validate_parameters=False; total time=   0.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=734, scale_pos_weight=2, seed=795, validate_parameters=False; total time=  37.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=734, scale_pos_weight=2, seed=795, validate_parameters=False; total time=  39.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=734, scale_pos_weight=2, seed=795, validate_parameters=False; total time=  36.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=734, scale_pos_weight=2, seed=795, validate_parameters=False; total time=  38.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=734, scale_pos_weight=2, seed=795, validate_parameters=False; total time=  37.5s\n",
      "[16:18:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=640, scale_pos_weight=2, seed=576, validate_parameters=True; total time=   0.1s\n",
      "[16:18:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=640, scale_pos_weight=2, seed=576, validate_parameters=True; total time=   0.1s\n",
      "[16:18:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=640, scale_pos_weight=2, seed=576, validate_parameters=True; total time=   0.1s\n",
      "[16:18:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=640, scale_pos_weight=2, seed=576, validate_parameters=True; total time=   0.1s\n",
      "[16:18:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=640, scale_pos_weight=2, seed=576, validate_parameters=True; total time=   0.1s\n",
      "[16:18:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=181, scale_pos_weight=2, seed=507, validate_parameters=True; total time=   0.0s\n",
      "[16:18:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=181, scale_pos_weight=2, seed=507, validate_parameters=True; total time=   0.0s\n",
      "[16:18:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=181, scale_pos_weight=2, seed=507, validate_parameters=True; total time=   0.0s\n",
      "[16:18:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=181, scale_pos_weight=2, seed=507, validate_parameters=True; total time=   0.0s\n",
      "[16:18:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=181, scale_pos_weight=2, seed=507, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=707, scale_pos_weight=3, seed=683, validate_parameters=True; total time=  35.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=707, scale_pos_weight=3, seed=683, validate_parameters=True; total time=  35.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=707, scale_pos_weight=3, seed=683, validate_parameters=True; total time=  35.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=707, scale_pos_weight=3, seed=683, validate_parameters=True; total time=  44.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=707, scale_pos_weight=3, seed=683, validate_parameters=True; total time=  34.3s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=472, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[16:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=567, scale_pos_weight=2, seed=850, validate_parameters=True; total time=   0.1s\n",
      "[16:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=567, scale_pos_weight=2, seed=850, validate_parameters=True; total time=   0.1s\n",
      "[16:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=567, scale_pos_weight=2, seed=850, validate_parameters=True; total time=   0.1s\n",
      "[16:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=567, scale_pos_weight=2, seed=850, validate_parameters=True; total time=   0.1s\n",
      "[16:21:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=567, scale_pos_weight=2, seed=850, validate_parameters=True; total time=   0.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=501, scale_pos_weight=2, seed=21, validate_parameters=True; total time=  18.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=501, scale_pos_weight=2, seed=21, validate_parameters=True; total time=  17.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=501, scale_pos_weight=2, seed=21, validate_parameters=True; total time=  17.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=501, scale_pos_weight=2, seed=21, validate_parameters=True; total time=  17.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=501, scale_pos_weight=2, seed=21, validate_parameters=True; total time=  18.0s\n",
      "[16:23:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=234, scale_pos_weight=3, seed=133, validate_parameters=False; total time=   0.0s\n",
      "[16:23:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=234, scale_pos_weight=3, seed=133, validate_parameters=False; total time=   0.0s\n",
      "[16:23:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=234, scale_pos_weight=3, seed=133, validate_parameters=False; total time=   0.0s\n",
      "[16:23:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=234, scale_pos_weight=3, seed=133, validate_parameters=False; total time=   0.0s\n",
      "[16:23:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=234, scale_pos_weight=3, seed=133, validate_parameters=False; total time=   0.0s\n",
      "[16:23:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=439, scale_pos_weight=3, seed=539, validate_parameters=False; total time=   0.1s\n",
      "[16:23:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=439, scale_pos_weight=3, seed=539, validate_parameters=False; total time=   0.1s\n",
      "[16:23:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=439, scale_pos_weight=3, seed=539, validate_parameters=False; total time=   0.1s\n",
      "[16:23:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=439, scale_pos_weight=3, seed=539, validate_parameters=False; total time=   0.1s\n",
      "[16:23:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=439, scale_pos_weight=3, seed=539, validate_parameters=False; total time=   0.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=426, scale_pos_weight=3, seed=431, validate_parameters=False; total time=   1.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=426, scale_pos_weight=3, seed=431, validate_parameters=False; total time=   1.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=426, scale_pos_weight=3, seed=431, validate_parameters=False; total time=   1.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=426, scale_pos_weight=3, seed=431, validate_parameters=False; total time=   1.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=426, scale_pos_weight=3, seed=431, validate_parameters=False; total time=   1.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=273, scale_pos_weight=3, seed=147, validate_parameters=False; total time=   5.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=273, scale_pos_weight=3, seed=147, validate_parameters=False; total time=   5.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=273, scale_pos_weight=3, seed=147, validate_parameters=False; total time=   5.9s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=273, scale_pos_weight=3, seed=147, validate_parameters=False; total time=   5.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=273, scale_pos_weight=3, seed=147, validate_parameters=False; total time=   5.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=231, scale_pos_weight=2, seed=616, validate_parameters=False; total time=   4.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=231, scale_pos_weight=2, seed=616, validate_parameters=False; total time=   4.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=231, scale_pos_weight=2, seed=616, validate_parameters=False; total time=   4.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=231, scale_pos_weight=2, seed=616, validate_parameters=False; total time=   4.4s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=231, scale_pos_weight=2, seed=616, validate_parameters=False; total time=   4.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=263, scale_pos_weight=2, seed=439, validate_parameters=True; total time=   5.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=263, scale_pos_weight=2, seed=439, validate_parameters=True; total time=   5.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=263, scale_pos_weight=2, seed=439, validate_parameters=True; total time=   5.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=263, scale_pos_weight=2, seed=439, validate_parameters=True; total time=   5.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=263, scale_pos_weight=2, seed=439, validate_parameters=True; total time=   5.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=997, scale_pos_weight=3, seed=600, validate_parameters=False; total time= 1.1min\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=997, scale_pos_weight=3, seed=600, validate_parameters=False; total time= 1.1min\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=997, scale_pos_weight=3, seed=600, validate_parameters=False; total time= 1.1min\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=997, scale_pos_weight=3, seed=600, validate_parameters=False; total time= 1.1min\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=997, scale_pos_weight=3, seed=600, validate_parameters=False; total time= 1.1min\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=825, scale_pos_weight=3, seed=465, validate_parameters=False; total time=   2.7s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=825, scale_pos_weight=3, seed=465, validate_parameters=False; total time=   2.8s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=825, scale_pos_weight=3, seed=465, validate_parameters=False; total time=   2.7s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=825, scale_pos_weight=3, seed=465, validate_parameters=False; total time=   2.6s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=825, scale_pos_weight=3, seed=465, validate_parameters=False; total time=   2.7s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=546, scale_pos_weight=2, seed=992, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=546, scale_pos_weight=2, seed=992, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=546, scale_pos_weight=2, seed=992, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=546, scale_pos_weight=2, seed=992, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=546, scale_pos_weight=2, seed=992, validate_parameters=True; total time=   0.0s\n",
      "[16:30:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=849, scale_pos_weight=2, seed=352, validate_parameters=True; total time=   0.2s\n",
      "[16:30:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=849, scale_pos_weight=2, seed=352, validate_parameters=True; total time=   0.2s\n",
      "[16:30:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=849, scale_pos_weight=2, seed=352, validate_parameters=True; total time=   0.2s\n",
      "[16:30:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=849, scale_pos_weight=2, seed=352, validate_parameters=True; total time=   0.2s\n",
      "[16:30:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=849, scale_pos_weight=2, seed=352, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=113, scale_pos_weight=3, seed=453, validate_parameters=True; total time=   1.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=113, scale_pos_weight=3, seed=453, validate_parameters=True; total time=   1.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=113, scale_pos_weight=3, seed=453, validate_parameters=True; total time=   1.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=113, scale_pos_weight=3, seed=453, validate_parameters=True; total time=   1.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=113, scale_pos_weight=3, seed=453, validate_parameters=True; total time=   1.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=330, scale_pos_weight=3, seed=859, validate_parameters=False; total time=   1.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=330, scale_pos_weight=3, seed=859, validate_parameters=False; total time=   1.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=330, scale_pos_weight=3, seed=859, validate_parameters=False; total time=   1.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=330, scale_pos_weight=3, seed=859, validate_parameters=False; total time=   1.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=330, scale_pos_weight=3, seed=859, validate_parameters=False; total time=   1.2s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=230, scale_pos_weight=3, seed=519, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=230, scale_pos_weight=3, seed=519, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=230, scale_pos_weight=3, seed=519, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=230, scale_pos_weight=3, seed=519, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=230, scale_pos_weight=3, seed=519, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=759, scale_pos_weight=2, seed=538, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=759, scale_pos_weight=2, seed=538, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=759, scale_pos_weight=2, seed=538, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=759, scale_pos_weight=2, seed=538, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=759, scale_pos_weight=2, seed=538, validate_parameters=False; total time=   0.0s\n",
      "[16:30:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=840, scale_pos_weight=3, seed=800, validate_parameters=False; total time=   0.2s\n",
      "[16:30:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=840, scale_pos_weight=3, seed=800, validate_parameters=False; total time=   0.2s\n",
      "[16:30:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=840, scale_pos_weight=3, seed=800, validate_parameters=False; total time=   0.2s\n",
      "[16:30:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=840, scale_pos_weight=3, seed=800, validate_parameters=False; total time=   0.2s\n",
      "[16:30:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=840, scale_pos_weight=3, seed=800, validate_parameters=False; total time=   0.2s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=58, scale_pos_weight=3, seed=29, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=58, scale_pos_weight=3, seed=29, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=58, scale_pos_weight=3, seed=29, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=58, scale_pos_weight=3, seed=29, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=58, scale_pos_weight=3, seed=29, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=741, scale_pos_weight=2, seed=37, validate_parameters=True; total time=  38.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=741, scale_pos_weight=2, seed=37, validate_parameters=True; total time=  38.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=741, scale_pos_weight=2, seed=37, validate_parameters=True; total time=  37.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=741, scale_pos_weight=2, seed=37, validate_parameters=True; total time=  38.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=741, scale_pos_weight=2, seed=37, validate_parameters=True; total time=  38.4s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=845, scale_pos_weight=2, seed=636, validate_parameters=True; total time=  48.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=845, scale_pos_weight=2, seed=636, validate_parameters=True; total time=  49.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=845, scale_pos_weight=2, seed=636, validate_parameters=True; total time=  49.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=845, scale_pos_weight=2, seed=636, validate_parameters=True; total time=  48.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=845, scale_pos_weight=2, seed=636, validate_parameters=True; total time=  49.4s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=308, scale_pos_weight=3, seed=571, validate_parameters=False; total time=   7.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=308, scale_pos_weight=3, seed=571, validate_parameters=False; total time=   7.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=308, scale_pos_weight=3, seed=571, validate_parameters=False; total time=   7.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=308, scale_pos_weight=3, seed=571, validate_parameters=False; total time=   8.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=308, scale_pos_weight=3, seed=571, validate_parameters=False; total time=   7.0s\n",
      "[16:38:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=378, scale_pos_weight=3, seed=967, validate_parameters=True; total time=   0.0s\n",
      "[16:38:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=378, scale_pos_weight=3, seed=967, validate_parameters=True; total time=   0.0s\n",
      "[16:38:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=378, scale_pos_weight=3, seed=967, validate_parameters=True; total time=   0.0s\n",
      "[16:38:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=378, scale_pos_weight=3, seed=967, validate_parameters=True; total time=   0.0s\n",
      "[16:38:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=378, scale_pos_weight=3, seed=967, validate_parameters=True; total time=   0.0s\n",
      "[16:38:50] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=357, scale_pos_weight=3, seed=295, validate_parameters=False; total time=   0.0s\n",
      "[16:38:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=357, scale_pos_weight=3, seed=295, validate_parameters=False; total time=   0.0s\n",
      "[16:38:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=357, scale_pos_weight=3, seed=295, validate_parameters=False; total time=   0.0s\n",
      "[16:38:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=357, scale_pos_weight=3, seed=295, validate_parameters=False; total time=   0.0s\n",
      "[16:38:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=357, scale_pos_weight=3, seed=295, validate_parameters=False; total time=   0.0s\n",
      "[16:38:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=838, scale_pos_weight=2, seed=579, validate_parameters=True; total time=   0.2s\n",
      "[16:38:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=838, scale_pos_weight=2, seed=579, validate_parameters=True; total time=   0.2s\n",
      "[16:38:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=838, scale_pos_weight=2, seed=579, validate_parameters=True; total time=   0.2s\n",
      "[16:38:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=838, scale_pos_weight=2, seed=579, validate_parameters=True; total time=   0.2s\n",
      "[16:38:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=838, scale_pos_weight=2, seed=579, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=158, scale_pos_weight=2, seed=767, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=158, scale_pos_weight=2, seed=767, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=158, scale_pos_weight=2, seed=767, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=158, scale_pos_weight=2, seed=767, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=158, scale_pos_weight=2, seed=767, validate_parameters=False; total time=   2.4s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=411, scale_pos_weight=2, seed=148, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=411, scale_pos_weight=2, seed=148, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=411, scale_pos_weight=2, seed=148, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=411, scale_pos_weight=2, seed=148, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=411, scale_pos_weight=2, seed=148, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=256, scale_pos_weight=2, seed=245, validate_parameters=True; total time=   5.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=256, scale_pos_weight=2, seed=245, validate_parameters=True; total time=   5.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=256, scale_pos_weight=2, seed=245, validate_parameters=True; total time=   5.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=256, scale_pos_weight=2, seed=245, validate_parameters=True; total time=   5.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=256, scale_pos_weight=2, seed=245, validate_parameters=True; total time=   5.2s\n",
      "[16:39:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=789, scale_pos_weight=3, seed=835, validate_parameters=True; total time=   0.2s\n",
      "[16:39:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=789, scale_pos_weight=3, seed=835, validate_parameters=True; total time=   0.2s\n",
      "[16:39:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=789, scale_pos_weight=3, seed=835, validate_parameters=True; total time=   0.2s\n",
      "[16:39:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=789, scale_pos_weight=3, seed=835, validate_parameters=True; total time=   0.2s\n",
      "[16:39:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=789, scale_pos_weight=3, seed=835, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=3, seed=870, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=3, seed=870, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=3, seed=870, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=3, seed=870, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=3, seed=870, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=991, scale_pos_weight=3, seed=802, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=991, scale_pos_weight=3, seed=802, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=991, scale_pos_weight=3, seed=802, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=991, scale_pos_weight=3, seed=802, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=991, scale_pos_weight=3, seed=802, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=800, scale_pos_weight=3, seed=984, validate_parameters=False; total time=   2.6s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=800, scale_pos_weight=3, seed=984, validate_parameters=False; total time=   2.6s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=800, scale_pos_weight=3, seed=984, validate_parameters=False; total time=   2.7s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=800, scale_pos_weight=3, seed=984, validate_parameters=False; total time=   2.9s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=800, scale_pos_weight=3, seed=984, validate_parameters=False; total time=   2.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=757, scale_pos_weight=3, seed=688, validate_parameters=False; total time=  37.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=757, scale_pos_weight=3, seed=688, validate_parameters=False; total time=  39.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=757, scale_pos_weight=3, seed=688, validate_parameters=False; total time=  38.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=757, scale_pos_weight=3, seed=688, validate_parameters=False; total time=  38.9s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=757, scale_pos_weight=3, seed=688, validate_parameters=False; total time=  38.1s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=438, scale_pos_weight=2, seed=663, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=438, scale_pos_weight=2, seed=663, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=438, scale_pos_weight=2, seed=663, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=438, scale_pos_weight=2, seed=663, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=438, scale_pos_weight=2, seed=663, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=592, scale_pos_weight=2, seed=417, validate_parameters=True; total time=   2.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=592, scale_pos_weight=2, seed=417, validate_parameters=True; total time=   2.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=592, scale_pos_weight=2, seed=417, validate_parameters=True; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=592, scale_pos_weight=2, seed=417, validate_parameters=True; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=592, scale_pos_weight=2, seed=417, validate_parameters=True; total time=   2.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=719, scale_pos_weight=3, seed=477, validate_parameters=True; total time=   2.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=719, scale_pos_weight=3, seed=477, validate_parameters=True; total time=   2.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=719, scale_pos_weight=3, seed=477, validate_parameters=True; total time=   2.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=719, scale_pos_weight=3, seed=477, validate_parameters=True; total time=   2.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=719, scale_pos_weight=3, seed=477, validate_parameters=True; total time=   2.4s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=2, seed=137, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=2, seed=137, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=2, seed=137, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=2, seed=137, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=552, scale_pos_weight=2, seed=137, validate_parameters=False; total time=   0.0s\n",
      "[16:43:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=299, scale_pos_weight=3, seed=397, validate_parameters=True; total time=   0.0s\n",
      "[16:43:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=299, scale_pos_weight=3, seed=397, validate_parameters=True; total time=   0.0s\n",
      "[16:43:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=299, scale_pos_weight=3, seed=397, validate_parameters=True; total time=   0.0s\n",
      "[16:43:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=299, scale_pos_weight=3, seed=397, validate_parameters=True; total time=   0.0s\n",
      "[16:43:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=299, scale_pos_weight=3, seed=397, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=724, scale_pos_weight=3, seed=289, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=724, scale_pos_weight=3, seed=289, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=724, scale_pos_weight=3, seed=289, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=724, scale_pos_weight=3, seed=289, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=724, scale_pos_weight=3, seed=289, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=807, scale_pos_weight=2, seed=294, validate_parameters=False; total time=  45.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=807, scale_pos_weight=2, seed=294, validate_parameters=False; total time=  42.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=807, scale_pos_weight=2, seed=294, validate_parameters=False; total time=  44.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=807, scale_pos_weight=2, seed=294, validate_parameters=False; total time=  44.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=807, scale_pos_weight=2, seed=294, validate_parameters=False; total time=  44.3s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=19, scale_pos_weight=3, seed=330, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=19, scale_pos_weight=3, seed=330, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=19, scale_pos_weight=3, seed=330, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=19, scale_pos_weight=3, seed=330, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=19, scale_pos_weight=3, seed=330, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=1, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=1, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=1, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=1, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=1, scale_pos_weight=2, seed=694, validate_parameters=True; total time=   0.0s\n",
      "[16:47:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=862, scale_pos_weight=2, seed=526, validate_parameters=False; total time=   0.2s\n",
      "[16:47:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=862, scale_pos_weight=2, seed=526, validate_parameters=False; total time=   0.2s\n",
      "[16:47:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=862, scale_pos_weight=2, seed=526, validate_parameters=False; total time=   0.2s\n",
      "[16:47:07] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=862, scale_pos_weight=2, seed=526, validate_parameters=False; total time=   0.2s\n",
      "[16:47:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=862, scale_pos_weight=2, seed=526, validate_parameters=False; total time=   0.2s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=372, scale_pos_weight=2, seed=135, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=372, scale_pos_weight=2, seed=135, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=372, scale_pos_weight=2, seed=135, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=372, scale_pos_weight=2, seed=135, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=372, scale_pos_weight=2, seed=135, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=690, scale_pos_weight=3, seed=409, validate_parameters=False; total time=  33.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=690, scale_pos_weight=3, seed=409, validate_parameters=False; total time=  32.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=690, scale_pos_weight=3, seed=409, validate_parameters=False; total time=  33.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=690, scale_pos_weight=3, seed=409, validate_parameters=False; total time=  32.4s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=690, scale_pos_weight=3, seed=409, validate_parameters=False; total time=  33.7s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=202, scale_pos_weight=2, seed=45, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=202, scale_pos_weight=2, seed=45, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=202, scale_pos_weight=2, seed=45, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=202, scale_pos_weight=2, seed=45, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=202, scale_pos_weight=2, seed=45, validate_parameters=True; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=620, scale_pos_weight=3, seed=35, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=620, scale_pos_weight=3, seed=35, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=620, scale_pos_weight=3, seed=35, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=620, scale_pos_weight=3, seed=35, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=620, scale_pos_weight=3, seed=35, validate_parameters=False; total time=   0.0s\n",
      "[16:49:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=437, scale_pos_weight=3, seed=686, validate_parameters=True; total time=   0.1s\n",
      "[16:49:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=437, scale_pos_weight=3, seed=686, validate_parameters=True; total time=   0.1s\n",
      "[16:49:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=437, scale_pos_weight=3, seed=686, validate_parameters=True; total time=   0.1s\n",
      "[16:49:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=437, scale_pos_weight=3, seed=686, validate_parameters=True; total time=   0.1s\n",
      "[16:49:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=437, scale_pos_weight=3, seed=686, validate_parameters=True; total time=   0.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=558, scale_pos_weight=3, seed=927, validate_parameters=False; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=558, scale_pos_weight=3, seed=927, validate_parameters=False; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=558, scale_pos_weight=3, seed=927, validate_parameters=False; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=558, scale_pos_weight=3, seed=927, validate_parameters=False; total time=   2.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=558, scale_pos_weight=3, seed=927, validate_parameters=False; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=761, scale_pos_weight=2, seed=15, validate_parameters=False; total time=   2.6s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=761, scale_pos_weight=2, seed=15, validate_parameters=False; total time=   2.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=761, scale_pos_weight=2, seed=15, validate_parameters=False; total time=   2.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=761, scale_pos_weight=2, seed=15, validate_parameters=False; total time=   2.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=761, scale_pos_weight=2, seed=15, validate_parameters=False; total time=   2.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=687, scale_pos_weight=3, seed=996, validate_parameters=True; total time=   2.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=687, scale_pos_weight=3, seed=996, validate_parameters=True; total time=   2.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=687, scale_pos_weight=3, seed=996, validate_parameters=True; total time=   2.3s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=687, scale_pos_weight=3, seed=996, validate_parameters=True; total time=   2.3s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=687, scale_pos_weight=3, seed=996, validate_parameters=True; total time=   2.4s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=911, scale_pos_weight=2, seed=65, validate_parameters=False; total time=  57.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=911, scale_pos_weight=2, seed=65, validate_parameters=False; total time=  56.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=911, scale_pos_weight=2, seed=65, validate_parameters=False; total time=  55.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=911, scale_pos_weight=2, seed=65, validate_parameters=False; total time=  57.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=911, scale_pos_weight=2, seed=65, validate_parameters=False; total time=  58.0s\n",
      "[16:55:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=18, scale_pos_weight=2, seed=546, validate_parameters=False; total time=   0.0s\n",
      "[16:55:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=18, scale_pos_weight=2, seed=546, validate_parameters=False; total time=   0.0s\n",
      "[16:55:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=18, scale_pos_weight=2, seed=546, validate_parameters=False; total time=   0.0s\n",
      "[16:55:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=18, scale_pos_weight=2, seed=546, validate_parameters=False; total time=   0.0s\n",
      "[16:55:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=18, scale_pos_weight=2, seed=546, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=651, scale_pos_weight=2, seed=558, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=651, scale_pos_weight=2, seed=558, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=651, scale_pos_weight=2, seed=558, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=651, scale_pos_weight=2, seed=558, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=651, scale_pos_weight=2, seed=558, validate_parameters=False; total time=   2.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=114, scale_pos_weight=2, seed=305, validate_parameters=True; total time=   0.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=114, scale_pos_weight=2, seed=305, validate_parameters=True; total time=   0.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=114, scale_pos_weight=2, seed=305, validate_parameters=True; total time=   0.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=114, scale_pos_weight=2, seed=305, validate_parameters=True; total time=   0.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=114, scale_pos_weight=2, seed=305, validate_parameters=True; total time=   0.4s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=768, scale_pos_weight=2, seed=521, validate_parameters=False; total time=   2.6s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=768, scale_pos_weight=2, seed=521, validate_parameters=False; total time=   2.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=768, scale_pos_weight=2, seed=521, validate_parameters=False; total time=   2.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=768, scale_pos_weight=2, seed=521, validate_parameters=False; total time=   2.6s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=768, scale_pos_weight=2, seed=521, validate_parameters=False; total time=   2.5s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=266, scale_pos_weight=2, seed=908, validate_parameters=True; total time=   1.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=266, scale_pos_weight=2, seed=908, validate_parameters=True; total time=   1.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=266, scale_pos_weight=2, seed=908, validate_parameters=True; total time=   1.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=266, scale_pos_weight=2, seed=908, validate_parameters=True; total time=   1.2s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=266, scale_pos_weight=2, seed=908, validate_parameters=True; total time=   1.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=253, scale_pos_weight=2, seed=710, validate_parameters=False; total time=   1.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=253, scale_pos_weight=2, seed=710, validate_parameters=False; total time=   1.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=253, scale_pos_weight=2, seed=710, validate_parameters=False; total time=   1.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=253, scale_pos_weight=2, seed=710, validate_parameters=False; total time=   1.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=253, scale_pos_weight=2, seed=710, validate_parameters=False; total time=   1.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=601, scale_pos_weight=3, seed=232, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=601, scale_pos_weight=3, seed=232, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=601, scale_pos_weight=3, seed=232, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=601, scale_pos_weight=3, seed=232, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=601, scale_pos_weight=3, seed=232, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=865, scale_pos_weight=3, seed=207, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=865, scale_pos_weight=3, seed=207, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=865, scale_pos_weight=3, seed=207, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=865, scale_pos_weight=3, seed=207, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree and dart, eval_metric=logloss, n_estimators=865, scale_pos_weight=3, seed=207, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=785, scale_pos_weight=3, seed=448, validate_parameters=False; total time=  44.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=785, scale_pos_weight=3, seed=448, validate_parameters=False; total time=  43.5s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=785, scale_pos_weight=3, seed=448, validate_parameters=False; total time=  41.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=785, scale_pos_weight=3, seed=448, validate_parameters=False; total time=  41.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=785, scale_pos_weight=3, seed=448, validate_parameters=False; total time=  42.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=570, scale_pos_weight=3, seed=295, validate_parameters=True; total time=   2.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=570, scale_pos_weight=3, seed=295, validate_parameters=True; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=570, scale_pos_weight=3, seed=295, validate_parameters=True; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=570, scale_pos_weight=3, seed=295, validate_parameters=True; total time=   2.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=570, scale_pos_weight=3, seed=295, validate_parameters=True; total time=   2.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=244, scale_pos_weight=3, seed=130, validate_parameters=False; total time=   4.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=244, scale_pos_weight=3, seed=130, validate_parameters=False; total time=   4.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=244, scale_pos_weight=3, seed=130, validate_parameters=False; total time=   4.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=244, scale_pos_weight=3, seed=130, validate_parameters=False; total time=   4.7s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=244, scale_pos_weight=3, seed=130, validate_parameters=False; total time=   4.7s\n",
      "[17:00:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=692, scale_pos_weight=3, seed=530, validate_parameters=True; total time=   0.2s\n",
      "[17:00:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=692, scale_pos_weight=3, seed=530, validate_parameters=True; total time=   0.2s\n",
      "[17:00:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=692, scale_pos_weight=3, seed=530, validate_parameters=True; total time=   0.1s\n",
      "[17:00:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=692, scale_pos_weight=3, seed=530, validate_parameters=True; total time=   0.1s\n",
      "[17:00:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=692, scale_pos_weight=3, seed=530, validate_parameters=True; total time=   0.1s\n",
      "[17:00:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=635, scale_pos_weight=3, seed=555, validate_parameters=True; total time=   0.1s\n",
      "[17:00:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=635, scale_pos_weight=3, seed=555, validate_parameters=True; total time=   0.1s\n",
      "[17:00:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=635, scale_pos_weight=3, seed=555, validate_parameters=True; total time=   0.1s\n",
      "[17:00:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=635, scale_pos_weight=3, seed=555, validate_parameters=True; total time=   0.1s\n",
      "[17:00:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=635, scale_pos_weight=3, seed=555, validate_parameters=True; total time=   0.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=37, scale_pos_weight=2, seed=738, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=37, scale_pos_weight=2, seed=738, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=37, scale_pos_weight=2, seed=738, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=37, scale_pos_weight=2, seed=738, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=37, scale_pos_weight=2, seed=738, validate_parameters=True; total time=   0.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=749, scale_pos_weight=3, seed=268, validate_parameters=False; total time=  39.3s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=749, scale_pos_weight=3, seed=268, validate_parameters=False; total time=  38.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=749, scale_pos_weight=3, seed=268, validate_parameters=False; total time=  38.8s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=749, scale_pos_weight=3, seed=268, validate_parameters=False; total time=  38.6s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=749, scale_pos_weight=3, seed=268, validate_parameters=False; total time=  39.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=157, scale_pos_weight=3, seed=629, validate_parameters=True; total time=   2.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=157, scale_pos_weight=3, seed=629, validate_parameters=True; total time=   2.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=157, scale_pos_weight=3, seed=629, validate_parameters=True; total time=   2.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=157, scale_pos_weight=3, seed=629, validate_parameters=True; total time=   2.2s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=157, scale_pos_weight=3, seed=629, validate_parameters=True; total time=   2.1s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=18, scale_pos_weight=3, seed=964, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=18, scale_pos_weight=3, seed=964, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=18, scale_pos_weight=3, seed=964, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=18, scale_pos_weight=3, seed=964, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=dart, eval_metric=logloss, n_estimators=18, scale_pos_weight=3, seed=964, validate_parameters=False; total time=   0.0s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=31, scale_pos_weight=3, seed=581, validate_parameters=True; total time=   0.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=31, scale_pos_weight=3, seed=581, validate_parameters=True; total time=   0.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=31, scale_pos_weight=3, seed=581, validate_parameters=True; total time=   0.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=31, scale_pos_weight=3, seed=581, validate_parameters=True; total time=   0.1s\n",
      "[CV] END booster=gbtree, eval_metric=logloss, n_estimators=31, scale_pos_weight=3, seed=581, validate_parameters=True; total time=   0.1s\n",
      "[17:03:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=894, scale_pos_weight=2, seed=55, validate_parameters=False; total time=   0.2s\n",
      "[17:03:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=894, scale_pos_weight=2, seed=55, validate_parameters=False; total time=   0.2s\n",
      "[17:03:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=894, scale_pos_weight=2, seed=55, validate_parameters=False; total time=   0.2s\n",
      "[17:03:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=894, scale_pos_weight=2, seed=55, validate_parameters=False; total time=   0.2s\n",
      "[17:03:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] END booster=gblinear, eval_metric=logloss, n_estimators=894, scale_pos_weight=2, seed=55, validate_parameters=False; total time=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1,\n",
       "                                           enable_categorical=False, gamma=0,\n",
       "                                           gpu_id=-1, importance_type=None,\n",
       "                                           interaction_constraints=&#x27;&#x27;,\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_delta_step=0, max_depth=6,\n",
       "                                           min_child_weight=1, missing=nan,\n",
       "                                           monotone_constraints=&#x27;()&#x27;,\n",
       "                                           n_estimat...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        &#x27;validate_parameters&#x27;: [True, False]},\n",
       "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1,\n",
       "                                           enable_categorical=False, gamma=0,\n",
       "                                           gpu_id=-1, importance_type=None,\n",
       "                                           interaction_constraints=&#x27;&#x27;,\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_delta_step=0, max_depth=6,\n",
       "                                           min_child_weight=1, missing=nan,\n",
       "                                           monotone_constraints=&#x27;()&#x27;,\n",
       "                                           n_estimat...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        &#x27;validate_parameters&#x27;: [True, False]},\n",
       "                   verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=4,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=4,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1,\n",
       "                                           enable_categorical=False, gamma=0,\n",
       "                                           gpu_id=-1, importance_type=None,\n",
       "                                           interaction_constraints='',\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_delta_step=0, max_depth=6,\n",
       "                                           min_child_weight=1, missing=nan,\n",
       "                                           monotone_constraints='()',\n",
       "                                           n_estimat...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        'validate_parameters': [True, False]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros: {'validate_parameters': True, 'seed': 37, 'scale_pos_weight': 2, 'n_estimators': 741, 'eval_metric': 'logloss', 'booster': 'dart'}\n",
      "Mejor Score: 0.6457171251750463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Mejores parametros: \"+str(model.best_params_))\n",
    "print(\"Mejor Score: \"+str(model.best_score_)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_validate_parameters</th>\n",
       "      <th>param_seed</th>\n",
       "      <th>param_scale_pos_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_eval_metric</th>\n",
       "      <th>param_booster</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>38.332483</td>\n",
       "      <td>0.335096</td>\n",
       "      <td>0.124914</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>True</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>741</td>\n",
       "      <td>logloss</td>\n",
       "      <td>dart</td>\n",
       "      <td>{'validate_parameters': True, 'seed': 37, 'scale_pos_weight': 2, 'n_estimators': 741, 'eval_metric': 'logloss', 'booster': 'dart'}</td>\n",
       "      <td>0.651805</td>\n",
       "      <td>0.602972</td>\n",
       "      <td>0.664544</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.663830</td>\n",
       "      <td>0.645717</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.184376</td>\n",
       "      <td>0.070817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>417</td>\n",
       "      <td>2</td>\n",
       "      <td>592</td>\n",
       "      <td>logloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>{'validate_parameters': True, 'seed': 417, 'scale_pos_weight': 2, 'n_estimators': 592, 'eval_metric': 'logloss', 'booster': 'gbtree'}</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>0.602972</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.663830</td>\n",
       "      <td>0.645292</td>\n",
       "      <td>0.022789</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>44.177220</td>\n",
       "      <td>0.897164</td>\n",
       "      <td>0.128054</td>\n",
       "      <td>0.006269</td>\n",
       "      <td>False</td>\n",
       "      <td>294</td>\n",
       "      <td>2</td>\n",
       "      <td>807</td>\n",
       "      <td>logloss</td>\n",
       "      <td>dart</td>\n",
       "      <td>{'validate_parameters': False, 'seed': 294, 'scale_pos_weight': 2, 'n_estimators': 807, 'eval_metric': 'logloss', 'booster': 'dart'}</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>0.602972</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.663830</td>\n",
       "      <td>0.644443</td>\n",
       "      <td>0.022043</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.304479</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.012473</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>False</td>\n",
       "      <td>558</td>\n",
       "      <td>2</td>\n",
       "      <td>651</td>\n",
       "      <td>logloss</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>{'validate_parameters': False, 'seed': 558, 'scale_pos_weight': 2, 'n_estimators': 651, 'eval_metric': 'logloss', 'booster': 'gbtree'}</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.605096</td>\n",
       "      <td>0.664544</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.644442</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37.673448</td>\n",
       "      <td>0.970572</td>\n",
       "      <td>0.116242</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>False</td>\n",
       "      <td>795</td>\n",
       "      <td>2</td>\n",
       "      <td>734</td>\n",
       "      <td>logloss</td>\n",
       "      <td>dart</td>\n",
       "      <td>{'validate_parameters': False, 'seed': 795, 'scale_pos_weight': 2, 'n_estimators': 734, 'eval_metric': 'logloss', 'booster': 'dart'}</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>0.602972</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.663830</td>\n",
       "      <td>0.644019</td>\n",
       "      <td>0.022041</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "48      38.332483      0.335096         0.124914        0.000011   \n",
       "63       2.184376      0.070817         0.000000        0.000000   \n",
       "68      44.177220      0.897164         0.128054        0.006269   \n",
       "82       2.304479      0.011710         0.012473        0.006236   \n",
       "25      37.673448      0.970572         0.116242        0.007211   \n",
       "\n",
       "   param_validate_parameters param_seed param_scale_pos_weight  \\\n",
       "48                      True         37                      2   \n",
       "63                      True        417                      2   \n",
       "68                     False        294                      2   \n",
       "82                     False        558                      2   \n",
       "25                     False        795                      2   \n",
       "\n",
       "   param_n_estimators param_eval_metric param_booster  \\\n",
       "48                741           logloss          dart   \n",
       "63                592           logloss        gbtree   \n",
       "68                807           logloss          dart   \n",
       "82                651           logloss        gbtree   \n",
       "25                734           logloss          dart   \n",
       "\n",
       "                                                                                                                                    params  \\\n",
       "48      {'validate_parameters': True, 'seed': 37, 'scale_pos_weight': 2, 'n_estimators': 741, 'eval_metric': 'logloss', 'booster': 'dart'}   \n",
       "63   {'validate_parameters': True, 'seed': 417, 'scale_pos_weight': 2, 'n_estimators': 592, 'eval_metric': 'logloss', 'booster': 'gbtree'}   \n",
       "68    {'validate_parameters': False, 'seed': 294, 'scale_pos_weight': 2, 'n_estimators': 807, 'eval_metric': 'logloss', 'booster': 'dart'}   \n",
       "82  {'validate_parameters': False, 'seed': 558, 'scale_pos_weight': 2, 'n_estimators': 651, 'eval_metric': 'logloss', 'booster': 'gbtree'}   \n",
       "25    {'validate_parameters': False, 'seed': 795, 'scale_pos_weight': 2, 'n_estimators': 734, 'eval_metric': 'logloss', 'booster': 'dart'}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "48           0.651805           0.602972           0.664544   \n",
       "63           0.647558           0.602972           0.666667   \n",
       "68           0.647558           0.602972           0.662420   \n",
       "82           0.643312           0.605096           0.664544   \n",
       "25           0.647558           0.602972           0.662420   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "48           0.645435           0.663830         0.645717        0.022569   \n",
       "63           0.645435           0.663830         0.645292        0.022789   \n",
       "68           0.645435           0.663830         0.644443        0.022043   \n",
       "82           0.647558           0.661702         0.644442        0.021266   \n",
       "25           0.643312           0.663830         0.644019        0.022041   \n",
       "\n",
       "    rank_test_score  \n",
       "48                1  \n",
       "63                2  \n",
       "68                3  \n",
       "82                4  \n",
       "25                5  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veamos los resultados obtenidos\n",
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scores.sort_values(\"mean_test_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ejecuta la predicción de resultados con X_test\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.629881154499151\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[146 123]\n",
      " [ 95 225]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier is: 0.629881154499151\n",
      "Precision Score of the classifier is: 0.646551724137931\n",
      "Recall Score of the classifier is: 0.703125\n",
      "F1 Score of the classifier is: 0.6736526946107785\n",
      "AUC for our classifier is: 0.6722583643122677\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(f\"Accuracy of the classifier is: {accuracy_score(y_test, predictions)}\")\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "print(f\"Precision Score of the classifier is: {precision_score(y_test, predictions)}\")\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(f\"Recall Score of the classifier is: {recall_score(y_test, predictions)}\")\n",
    "\n",
    "# Calculo del F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 Score of the classifier is: {f1_score(y_test, predictions)}\")\n",
    "\n",
    "#Template CURVA - ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "class_probabilities = model.predict_proba(X_test)\n",
    "preds = class_probabilities[:, 1]\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# AUC\n",
    "print(f\"AUC for our classifier is: {roc_auc}\")\n",
    "\n",
    "# Gráfica de la Curva ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Al modelo le cuesta detectar las clases (hay mucho falsos positivos y negativos). \n",
    "- Detecta mucho mejor los verdaderos positivos (el resto de los modelos tenían altos falsos positivos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen X e y\n",
    "X = BTC_metals_2.drop (['Target'],axis=1)\n",
    "y = BTC_metals_2 ['Target']\n",
    "\n",
    "# Dividimos los datos en Train y Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye la grilla de RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definelos hyperparámetros\n",
    "param_grid = {'boosting_type': ['gbdt','dart','goss', 'rf'],\n",
    "              'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'class_weight': ['balanced'] ,\n",
    "              'n_estimators':[100, 1000, 10000],\n",
    "              'subsample':[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.99],\n",
    "              'random_state': np.arange (1,1000,1)\n",
    "              }\n",
    "\n",
    "# Se uitiliza la RandomizedSearchCV y se agrega Cross validation con k=5 \n",
    "model = RandomizedSearchCV(clf, param_grid, cv=5, n_iter=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=444, subsample=1e-05; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=444, subsample=1e-05; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=444, subsample=1e-05; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=444, subsample=1e-05; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=444, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=83, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=83, subsample=0.01; total time=   0.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=83, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=83, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=83, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=739, subsample=0.0001; total time=   0.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=739, subsample=0.0001; total time=   0.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=739, subsample=0.0001; total time=   0.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=739, subsample=0.0001; total time=   0.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=739, subsample=0.0001; total time=   0.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=101, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=101, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=101, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=101, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=101, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=125, subsample=0.01; total time=   2.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=125, subsample=0.01; total time=   2.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=125, subsample=0.01; total time=   2.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=125, subsample=0.01; total time=   1.8s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=125, subsample=0.01; total time=   1.9s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=334, subsample=0.1; total time=  15.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=334, subsample=0.1; total time=  14.8s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=334, subsample=0.1; total time=  14.4s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=334, subsample=0.1; total time=  14.8s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=334, subsample=0.1; total time=  14.8s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=599, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=599, subsample=0.0001; total time=   0.2s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=599, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=599, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=599, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=43, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=43, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=43, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=43, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=43, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=73, subsample=0.0001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=73, subsample=0.0001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=73, subsample=0.0001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=73, subsample=0.0001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=73, subsample=0.0001; total time=   0.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=413, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=413, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=413, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=413, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=413, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=418, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=418, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=418, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=418, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=418, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=898, subsample=0.1; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=898, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=898, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=898, subsample=0.1; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=898, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=433, subsample=1e-05; total time=   3.4s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=433, subsample=1e-05; total time=   3.5s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=433, subsample=1e-05; total time=   3.5s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=433, subsample=1e-05; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=433, subsample=1e-05; total time=   3.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=915, subsample=0.0001; total time=  13.8s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=915, subsample=0.0001; total time=  14.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=915, subsample=0.0001; total time=  13.9s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=915, subsample=0.0001; total time=  14.2s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=915, subsample=0.0001; total time=  14.3s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=678, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=678, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=678, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=678, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=678, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=596, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=596, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=596, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=596, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=596, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=149, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=149, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=149, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=149, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=149, subsample=0.0001; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=440, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=440, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=440, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=440, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=440, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=63, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=63, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=63, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=63, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=63, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=673, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=673, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=673, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=673, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=673, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=251, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=251, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=251, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=251, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=251, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=228, subsample=1e-05; total time=   5.3s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=228, subsample=1e-05; total time=   5.3s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=228, subsample=1e-05; total time=   5.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=228, subsample=1e-05; total time=   5.6s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=228, subsample=1e-05; total time=   5.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=667, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=667, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=667, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=667, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=667, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=382, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=382, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=382, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=382, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=382, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=129, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=129, subsample=0.001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=129, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=129, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=129, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=10000, random_state=508, subsample=1e-05; total time=   2.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=10000, random_state=508, subsample=1e-05; total time=   2.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=10000, random_state=508, subsample=1e-05; total time=   2.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=10000, random_state=508, subsample=1e-05; total time=   1.9s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=10000, random_state=508, subsample=1e-05; total time=   2.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=413, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=413, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=413, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=413, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=413, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=548, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=548, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=548, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=548, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=548, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=242, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=242, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=242, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=242, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=242, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=781, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=781, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=781, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=781, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=781, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=429, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=429, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=429, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=429, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=429, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=976, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=976, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=976, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=976, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=976, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=2, subsample=0.1; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=2, subsample=0.1; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=2, subsample=0.1; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=2, subsample=0.1; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=2, subsample=0.1; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=925, subsample=0.001; total time=   2.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=925, subsample=0.001; total time=   2.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=925, subsample=0.001; total time=   2.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=925, subsample=0.001; total time=   2.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=925, subsample=0.001; total time=   2.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=690, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=690, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=690, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=690, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=690, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=211, subsample=0.001; total time=  13.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=211, subsample=0.001; total time=  13.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=211, subsample=0.001; total time=  14.6s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=211, subsample=0.001; total time=  13.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=211, subsample=0.001; total time=  13.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=183, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=183, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=183, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=183, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=183, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=132, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=132, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=132, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=132, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=132, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=922, subsample=0.001; total time=   1.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=922, subsample=0.001; total time=   1.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=922, subsample=0.001; total time=   1.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=922, subsample=0.001; total time=   1.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=922, subsample=0.001; total time=   1.5s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=670, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=670, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=670, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=670, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=670, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=939, subsample=0.0001; total time=   0.9s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=939, subsample=0.0001; total time=   0.9s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=939, subsample=0.0001; total time=   1.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=939, subsample=0.0001; total time=   1.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=939, subsample=0.0001; total time=   1.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=160, subsample=0.01; total time=   0.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=160, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=160, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=160, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=160, subsample=0.01; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=106, subsample=0.01; total time=   1.4s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=106, subsample=0.01; total time=   1.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=106, subsample=0.01; total time=   1.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=106, subsample=0.01; total time=   1.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.01, n_estimators=1000, random_state=106, subsample=0.01; total time=   1.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=885, subsample=0.01; total time=  43.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=885, subsample=0.01; total time=  40.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=885, subsample=0.01; total time=  41.7s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=885, subsample=0.01; total time=  40.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=885, subsample=0.01; total time=  41.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=958, subsample=0.0001; total time=   1.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=958, subsample=0.0001; total time=   1.3s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=958, subsample=0.0001; total time=   1.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=958, subsample=0.0001; total time=   1.3s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=958, subsample=0.0001; total time=   1.3s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=933, subsample=0.01; total time=   0.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=933, subsample=0.01; total time=   0.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=933, subsample=0.01; total time=   0.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=933, subsample=0.01; total time=   0.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=933, subsample=0.01; total time=   0.4s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=237, subsample=0.1; total time=  41.9s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=237, subsample=0.1; total time=  40.9s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=237, subsample=0.1; total time=  41.7s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=237, subsample=0.1; total time=  40.8s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=237, subsample=0.1; total time=  43.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=158, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=158, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=158, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=158, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=158, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=631, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=631, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=631, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=631, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=631, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=128, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=128, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=128, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=128, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=100, random_state=128, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=375, subsample=0.0001; total time=   2.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=375, subsample=0.0001; total time=   2.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=375, subsample=0.0001; total time=   2.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=375, subsample=0.0001; total time=   2.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=375, subsample=0.0001; total time=   2.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=882, subsample=0.0001; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=882, subsample=0.0001; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=882, subsample=0.0001; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=882, subsample=0.0001; total time=   3.6s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=882, subsample=0.0001; total time=   3.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=314, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=314, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=314, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=314, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=314, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=150, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=150, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=150, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=150, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=150, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=349, subsample=0.1; total time=   1.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=349, subsample=0.1; total time=   1.6s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=349, subsample=0.1; total time=   1.8s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=349, subsample=0.1; total time=   1.8s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=349, subsample=0.1; total time=   1.7s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=46, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=46, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=46, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=46, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=46, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=328, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=328, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=328, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=328, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=328, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=250, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=250, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=250, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=250, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=250, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=250, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=250, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=250, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=250, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=250, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=120, subsample=0.001; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=120, subsample=0.001; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=120, subsample=0.001; total time=   3.4s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=120, subsample=0.001; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=120, subsample=0.001; total time=   3.6s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=878, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=878, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=878, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=878, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=878, subsample=0.99; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=854, subsample=0.99; total time=  30.9s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=854, subsample=0.99; total time=  32.7s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=854, subsample=0.99; total time=  31.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=854, subsample=0.99; total time=  32.7s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=854, subsample=0.99; total time=  30.8s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=929, subsample=0.001; total time=  13.8s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=929, subsample=0.001; total time=  15.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=929, subsample=0.001; total time=  14.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=929, subsample=0.001; total time=  13.6s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=929, subsample=0.001; total time=  14.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=169, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=169, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=169, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=169, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1000, n_estimators=100, random_state=169, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=839, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=839, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=839, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=839, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=839, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=476, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=476, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=476, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=476, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=476, subsample=0.001; total time=   0.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=800, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=800, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=800, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=800, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=800, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=475, subsample=0.99; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=475, subsample=0.99; total time=   3.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=475, subsample=0.99; total time=   3.9s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=475, subsample=0.99; total time=   3.7s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=475, subsample=0.99; total time=   3.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=902, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=902, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=902, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=902, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=902, subsample=0.0001; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=973, subsample=0.0001; total time=  14.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=973, subsample=0.0001; total time=  14.6s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=973, subsample=0.0001; total time=  14.4s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=973, subsample=0.0001; total time=  14.6s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=973, subsample=0.0001; total time=  14.9s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=208, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=208, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=208, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=208, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.01, n_estimators=100, random_state=208, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=209, subsample=1e-05; total time=  11.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=209, subsample=1e-05; total time=  13.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=209, subsample=1e-05; total time=  15.5s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=209, subsample=1e-05; total time=  13.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=209, subsample=1e-05; total time=  13.3s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=848, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=848, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=848, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=848, subsample=0.99; total time=   0.1s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=848, subsample=0.99; total time=   0.2s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=100, random_state=339, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=100, random_state=339, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=100, random_state=339, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=100, random_state=339, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=100, random_state=339, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=211, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=211, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=211, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=211, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=211, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=202, subsample=0.01; total time=   4.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=202, subsample=0.01; total time=   3.9s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=202, subsample=0.01; total time=   3.9s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=202, subsample=0.01; total time=   3.8s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=202, subsample=0.01; total time=   3.7s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=664, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=664, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=664, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=664, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=1000, random_state=664, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=8, subsample=0.0001; total time=   1.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=8, subsample=0.0001; total time=   1.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=8, subsample=0.0001; total time=   1.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=8, subsample=0.0001; total time=   1.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=8, subsample=0.0001; total time=   1.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=902, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=902, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=902, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=902, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=902, subsample=1e-05; total time=   0.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=494, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=494, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=494, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=494, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=10000, random_state=494, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=604, subsample=0.01; total time=   1.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=604, subsample=0.01; total time=   1.6s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=604, subsample=0.01; total time=   1.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=604, subsample=0.01; total time=   1.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=10, n_estimators=10000, random_state=604, subsample=0.01; total time=   1.6s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=934, subsample=0.0001; total time=   1.3s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=934, subsample=0.0001; total time=   1.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=934, subsample=0.0001; total time=   0.9s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=934, subsample=0.0001; total time=   0.9s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.1, n_estimators=1000, random_state=934, subsample=0.0001; total time=   1.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=574, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=574, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=574, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=574, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=574, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=480, subsample=0.99; total time=  43.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=480, subsample=0.99; total time=  44.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=480, subsample=0.99; total time=  41.9s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=480, subsample=0.99; total time=  43.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=480, subsample=0.99; total time=  43.9s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=16, subsample=0.1; total time=   1.5s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=16, subsample=0.1; total time=   1.5s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=16, subsample=0.1; total time=   1.4s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=16, subsample=0.1; total time=   1.4s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=1000, random_state=16, subsample=0.1; total time=   1.4s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=28, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=28, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=28, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=28, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=10, n_estimators=1000, random_state=28, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=699, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=699, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=699, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=699, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=699, subsample=0.001; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=473, subsample=0.001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=473, subsample=0.001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=473, subsample=0.001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=473, subsample=0.001; total time=   0.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=1, n_estimators=1000, random_state=473, subsample=0.001; total time=   0.2s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=997, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=997, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=997, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=997, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=10000, random_state=997, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=370, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=370, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=370, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=370, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.001, n_estimators=100, random_state=370, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=918, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=918, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=918, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=918, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=0.0001, n_estimators=1000, random_state=918, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=375, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=375, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=375, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=375, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=10, n_estimators=100, random_state=375, subsample=0.01; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=741, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=741, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=741, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=741, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=100, n_estimators=100, random_state=741, subsample=1e-05; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=224, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=224, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=224, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=224, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=rf, class_weight=balanced, learning_rate=1, n_estimators=100, random_state=224, subsample=0.1; total time=   0.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=854, subsample=1e-05; total time=  13.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=854, subsample=1e-05; total time=  13.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=854, subsample=1e-05; total time=  14.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=854, subsample=1e-05; total time=  13.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=854, subsample=1e-05; total time=  13.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=683, subsample=0.99; total time=  42.7s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=683, subsample=0.99; total time=  39.4s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=683, subsample=0.99; total time=  41.2s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=683, subsample=0.99; total time=  39.8s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.0001, n_estimators=10000, random_state=683, subsample=0.99; total time=  41.3s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=58, subsample=0.0001; total time=  12.5s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=58, subsample=0.0001; total time=  11.3s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=58, subsample=0.0001; total time=  12.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=58, subsample=0.0001; total time=  11.2s\n",
      "[CV] END boosting_type=goss, class_weight=balanced, learning_rate=0.001, n_estimators=10000, random_state=58, subsample=0.0001; total time=  11.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=989, subsample=0.0001; total time=  46.6s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=989, subsample=0.0001; total time=  46.3s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=989, subsample=0.0001; total time=  48.6s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=989, subsample=0.0001; total time=  49.1s\n",
      "[CV] END boosting_type=dart, class_weight=balanced, learning_rate=0.1, n_estimators=10000, random_state=989, subsample=0.0001; total time=  46.0s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=809, subsample=0.1; total time=  12.9s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=809, subsample=0.1; total time=  12.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=809, subsample=0.1; total time=  12.5s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=809, subsample=0.1; total time=  12.7s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=0.01, n_estimators=10000, random_state=809, subsample=0.1; total time=  13.4s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=350, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=350, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=350, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=350, subsample=0.1; total time=   0.1s\n",
      "[CV] END boosting_type=gbdt, class_weight=balanced, learning_rate=1000, n_estimators=1000, random_state=350, subsample=0.1; total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=LGBMClassifier(), n_iter=100,\n",
       "                   param_distributions={&#x27;boosting_type&#x27;: [&#x27;gbdt&#x27;, &#x27;dart&#x27;,\n",
       "                                                          &#x27;goss&#x27;, &#x27;rf&#x27;],\n",
       "                                        &#x27;class_weight&#x27;: [&#x27;balanced&#x27;],\n",
       "                                        &#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 1000, 10000],\n",
       "                                        &#x27;random_state&#x27;: array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        &#x27;subsample&#x27;: [1e-05, 0.0001, 0.001,\n",
       "                                                      0.01, 0.1, 0.99]},\n",
       "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=LGBMClassifier(), n_iter=100,\n",
       "                   param_distributions={&#x27;boosting_type&#x27;: [&#x27;gbdt&#x27;, &#x27;dart&#x27;,\n",
       "                                                          &#x27;goss&#x27;, &#x27;rf&#x27;],\n",
       "                                        &#x27;class_weight&#x27;: [&#x27;balanced&#x27;],\n",
       "                                        &#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 1000, 10000],\n",
       "                                        &#x27;random_state&#x27;: array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        &#x27;subsample&#x27;: [1e-05, 0.0001, 0.001,\n",
       "                                                      0.01, 0.1, 0.99]},\n",
       "                   verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LGBMClassifier(), n_iter=100,\n",
       "                   param_distributions={'boosting_type': ['gbdt', 'dart',\n",
       "                                                          'goss', 'rf'],\n",
       "                                        'class_weight': ['balanced'],\n",
       "                                        'learning_rate': [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        'n_estimators': [100, 1000, 10000],\n",
       "                                        'random_state': array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,...\n",
       "       924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936,\n",
       "       937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
       "       950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962,\n",
       "       963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975,\n",
       "       976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
       "       989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]),\n",
       "                                        'subsample': [1e-05, 0.0001, 0.001,\n",
       "                                                      0.01, 0.1, 0.99]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros: {'subsample': 0.0001, 'random_state': 934, 'n_estimators': 1000, 'learning_rate': 0.1, 'class_weight': 'balanced', 'boosting_type': 'goss'}\n",
      "Mejor Score: 0.6610109770971676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Mejores parametros: \"+str(model.best_params_))\n",
    "print(\"Mejor Score: \"+str(model.best_score_)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_random_state</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_class_weight</th>\n",
       "      <th>param_boosting_type</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.121659</td>\n",
       "      <td>0.145185</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>7.136645e-07</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>934</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>goss</td>\n",
       "      <td>{'subsample': 0.0001, 'random_state': 934, 'n_estimators': 1000, 'learning_rate': 0.1, 'class_weight': 'balanced', 'boosting_type': 'goss'}</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>0.656051</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.661011</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>12.996259</td>\n",
       "      <td>1.347238</td>\n",
       "      <td>0.354159</td>\n",
       "      <td>8.439353e-02</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>209</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>balanced</td>\n",
       "      <td>goss</td>\n",
       "      <td>{'subsample': 1e-05, 'random_state': 209, 'n_estimators': 10000, 'learning_rate': 0.001, 'class_weight': 'balanced', 'boosting_type': 'goss'}</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.632696</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.655907</td>\n",
       "      <td>0.016773</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>11.536420</td>\n",
       "      <td>0.542883</td>\n",
       "      <td>0.281187</td>\n",
       "      <td>9.879479e-03</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>58</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>balanced</td>\n",
       "      <td>goss</td>\n",
       "      <td>{'subsample': 0.0001, 'random_state': 58, 'n_estimators': 10000, 'learning_rate': 0.001, 'class_weight': 'balanced', 'boosting_type': 'goss'}</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.641189</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.663830</td>\n",
       "      <td>0.655059</td>\n",
       "      <td>0.015496</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.141452</td>\n",
       "      <td>0.009844</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>8.155823e-04</td>\n",
       "      <td>0.99</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>balanced</td>\n",
       "      <td>goss</td>\n",
       "      <td>{'subsample': 0.99, 'random_state': 128, 'n_estimators': 100, 'learning_rate': 0.1, 'class_weight': 'balanced', 'boosting_type': 'goss'}</td>\n",
       "      <td>0.636943</td>\n",
       "      <td>0.630573</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.657447</td>\n",
       "      <td>0.652084</td>\n",
       "      <td>0.016252</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.265453</td>\n",
       "      <td>0.135175</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>8.662177e-07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>106</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>balanced</td>\n",
       "      <td>goss</td>\n",
       "      <td>{'subsample': 0.01, 'random_state': 106, 'n_estimators': 1000, 'learning_rate': 0.01, 'class_weight': 'balanced', 'boosting_type': 'goss'}</td>\n",
       "      <td>0.634820</td>\n",
       "      <td>0.641189</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.673036</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.650812</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "81       1.121659      0.145185         0.015622    7.136645e-07   \n",
       "71      12.996259      1.347238         0.354159    8.439353e-02   \n",
       "96      11.536420      0.542883         0.281187    9.879479e-03   \n",
       "49       0.141452      0.009844         0.000408    8.155823e-04   \n",
       "42       1.265453      0.135175         0.015622    8.662177e-07   \n",
       "\n",
       "   param_subsample param_random_state param_n_estimators param_learning_rate  \\\n",
       "81          0.0001                934               1000                 0.1   \n",
       "71         0.00001                209              10000               0.001   \n",
       "96          0.0001                 58              10000               0.001   \n",
       "49            0.99                128                100                 0.1   \n",
       "42            0.01                106               1000                0.01   \n",
       "\n",
       "   param_class_weight param_boosting_type  \\\n",
       "81           balanced                goss   \n",
       "71           balanced                goss   \n",
       "96           balanced                goss   \n",
       "49           balanced                goss   \n",
       "42           balanced                goss   \n",
       "\n",
       "                                                                                                                                           params  \\\n",
       "81    {'subsample': 0.0001, 'random_state': 934, 'n_estimators': 1000, 'learning_rate': 0.1, 'class_weight': 'balanced', 'boosting_type': 'goss'}   \n",
       "71  {'subsample': 1e-05, 'random_state': 209, 'n_estimators': 10000, 'learning_rate': 0.001, 'class_weight': 'balanced', 'boosting_type': 'goss'}   \n",
       "96  {'subsample': 0.0001, 'random_state': 58, 'n_estimators': 10000, 'learning_rate': 0.001, 'class_weight': 'balanced', 'boosting_type': 'goss'}   \n",
       "49       {'subsample': 0.99, 'random_state': 128, 'n_estimators': 100, 'learning_rate': 0.1, 'class_weight': 'balanced', 'boosting_type': 'goss'}   \n",
       "42     {'subsample': 0.01, 'random_state': 106, 'n_estimators': 1000, 'learning_rate': 0.01, 'class_weight': 'balanced', 'boosting_type': 'goss'}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "81           0.647558           0.656051           0.660297   \n",
       "71           0.660297           0.632696           0.643312   \n",
       "96           0.645435           0.641189           0.643312   \n",
       "49           0.636943           0.630573           0.660297   \n",
       "42           0.634820           0.641189           0.643312   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "81           0.660297           0.680851         0.661011        0.010956   \n",
       "71           0.681529           0.661702         0.655907        0.016773   \n",
       "96           0.681529           0.663830         0.655059        0.015496   \n",
       "49           0.675159           0.657447         0.652084        0.016252   \n",
       "42           0.673036           0.661702         0.650812        0.014263   \n",
       "\n",
       "    rank_test_score  \n",
       "81                1  \n",
       "71                2  \n",
       "96                3  \n",
       "49                4  \n",
       "42                5  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veamos los resultados obtenidos\n",
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scores.sort_values(\"mean_test_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ejecuta la predicción de resultados con X_test\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.633276740237691\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[160 116]\n",
      " [100 213]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier is: 0.633276740237691\n",
      "Precision Score of the classifier is: 0.6474164133738601\n",
      "Recall Score of the classifier is: 0.6805111821086262\n",
      "F1 Score of the classifier is: 0.6635514018691588\n",
      "AUC for our classifier is: 0.6716442098439598\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(f\"Accuracy of the classifier is: {accuracy_score(y_test, predictions)}\")\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "print(f\"Precision Score of the classifier is: {precision_score(y_test, predictions)}\")\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(f\"Recall Score of the classifier is: {recall_score(y_test, predictions)}\")\n",
    "\n",
    "# Calculo del F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 Score of the classifier is: {f1_score(y_test, predictions)}\")\n",
    "\n",
    "#Template CURVA - ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "class_probabilities = model.predict_proba(X_test)\n",
    "preds = class_probabilities[:, 1]\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# AUC\n",
    "print(f\"AUC for our classifier is: {roc_auc}\")\n",
    "\n",
    "# Gráfica de la Curva ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Al modelo le cuesta detectar las clases (hay mucho falsos positivos y negativos). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen X e y\n",
    "X = BTC_metals_2.drop (['Target'],axis=1)\n",
    "y = BTC_metals_2 ['Target']\n",
    "\n",
    "# Dividimos los datos en Train y Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dt = HistGradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HistGradientBoostingClassifier()"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6570458404074703"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "HGBC = HistGradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye la grilla de RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definelos hyperparámetros\n",
    "param_grid = {'loss':['log_loss', 'auto', 'binary_crossentropyv', 'categorical_crossentropy'],\n",
    "              'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'max_iter': [100,1000,10000],\n",
    "              'max_leaf_nodes': [1,15,31,65,90,150],\n",
    "              'l2_regularization': [0, 1, 5, 10, 50,100],\n",
    "              'warm_start': [False]}\n",
    "\n",
    "# Se uitiliza la RandomizedSearchCV y se agrega Cross validation con k=5 \n",
    "model = RandomizedSearchCV(HGBC, param_grid, cv=5, n_iter=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.8s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.7s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.6s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.7s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.8s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   1.1s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=log_loss, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=  59.6s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=log_loss, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time= 1.2min\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=log_loss, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time= 1.2min\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=log_loss, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time= 1.2min\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=log_loss, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time= 1.2min\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time= 1.0min\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=  57.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=  58.6s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=  57.1s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=  56.7s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.1s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   1.5s\n",
      "[CV] END l2_regularization=5, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   1.5s\n",
      "[CV] END l2_regularization=5, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   1.5s\n",
      "[CV] END l2_regularization=5, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   1.5s\n",
      "[CV] END l2_regularization=5, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   1.5s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=auto, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=auto, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=auto, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=auto, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=auto, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 2.0min\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 1.9min\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 1.9min\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 2.0min\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 2.0min\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time= 3.4min\n",
      "[CV] END l2_regularization=1, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time= 3.4min\n",
      "[CV] END l2_regularization=1, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time= 3.5min\n",
      "[CV] END l2_regularization=1, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time= 3.4min\n",
      "[CV] END l2_regularization=1, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time= 3.4min\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  11.8s\n",
      "[CV] END l2_regularization=10, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  11.8s\n",
      "[CV] END l2_regularization=10, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  11.8s\n",
      "[CV] END l2_regularization=10, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  12.4s\n",
      "[CV] END l2_regularization=10, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  11.8s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   2.1s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   2.1s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   2.1s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   2.1s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   2.1s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=  10.3s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=  10.1s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=  10.5s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=  10.7s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=  10.4s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   9.8s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   9.9s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   9.9s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   9.8s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=  10.4s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  10.8s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  10.7s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  10.7s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  10.7s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  10.7s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=  11.1s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=  10.2s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=  10.1s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=  10.1s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=  10.5s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.001, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  15.1s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  17.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  15.3s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  15.4s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  15.2s\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.7s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.8s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.8s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   2.1s\n",
      "[CV] END l2_regularization=5, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   2.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 2.0min\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 2.0min\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 2.1min\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 2.1min\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=auto, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time= 2.1min\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   1.1s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=5, learning_rate=10, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=auto, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=  10.7s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=auto, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=  10.4s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=auto, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=  10.8s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=auto, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=  12.2s\n",
      "[CV] END l2_regularization=1, learning_rate=10, loss=auto, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=  10.6s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.3s\n",
      "[CV] END l2_regularization=10, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.3s\n",
      "[CV] END l2_regularization=10, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.3s\n",
      "[CV] END l2_regularization=10, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.4s\n",
      "[CV] END l2_regularization=10, learning_rate=0.001, loss=auto, max_iter=100, max_leaf_nodes=150, warm_start=False; total time=   1.2s\n",
      "[CV] END l2_regularization=5, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  12.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  12.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  12.7s\n",
      "[CV] END l2_regularization=5, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  11.9s\n",
      "[CV] END l2_regularization=5, learning_rate=0.1, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=  12.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=65, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=100, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=100, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=100, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=100, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=100, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=10, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  14.6s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  13.4s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  14.2s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  14.5s\n",
      "[CV] END l2_regularization=10, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  13.7s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.1s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=auto, max_iter=1000, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=log_loss, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=log_loss, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=log_loss, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=log_loss, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   1.1s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=log_loss, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.9s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.01, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=auto, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time= 3.7min\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=auto, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time= 3.7min\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=auto, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time= 3.5min\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=auto, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time= 3.6min\n",
      "[CV] END l2_regularization=0, learning_rate=0.0001, loss=auto, max_iter=10000, max_leaf_nodes=150, warm_start=False; total time= 3.6min\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=100, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=50, learning_rate=1000, loss=auto, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.1, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=1, learning_rate=1000, loss=log_loss, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=10, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=1, loss=categorical_crossentropy, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.4s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.5s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.7s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.4s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.8s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.01, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=log_loss, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=log_loss, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=log_loss, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=log_loss, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=1000, loss=log_loss, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  13.9s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  14.7s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  14.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  14.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.0001, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=  14.2s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=10, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=10, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=10, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=10, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=10, loss=binary_crossentropyv, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=0.01, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=categorical_crossentropy, max_iter=1000, max_leaf_nodes=31, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=10, learning_rate=100, loss=log_loss, max_iter=10000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=1, learning_rate=0.0001, loss=log_loss, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=0.01, loss=log_loss, max_iter=1000, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.6s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.4s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.5s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.4s\n",
      "[CV] END l2_regularization=100, learning_rate=1000, loss=log_loss, max_iter=10000, max_leaf_nodes=31, warm_start=False; total time=   8.4s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=50, learning_rate=0.001, loss=binary_crossentropyv, max_iter=1000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.9s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.6s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=100, learning_rate=100, loss=log_loss, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   0.8s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   6.3s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   6.8s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   7.3s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   7.4s\n",
      "[CV] END l2_regularization=100, learning_rate=0.0001, loss=auto, max_iter=1000, max_leaf_nodes=150, warm_start=False; total time=   5.8s\n",
      "[CV] END l2_regularization=50, learning_rate=1, loss=auto, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=50, learning_rate=1, loss=auto, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=50, learning_rate=1, loss=auto, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=50, learning_rate=1, loss=auto, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=50, learning_rate=1, loss=auto, max_iter=100, max_leaf_nodes=15, warm_start=False; total time=   0.5s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=0, learning_rate=100, loss=categorical_crossentropy, max_iter=10000, max_leaf_nodes=15, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=1, learning_rate=1, loss=log_loss, max_iter=100, max_leaf_nodes=1, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=5, learning_rate=0.01, loss=binary_crossentropyv, max_iter=10000, max_leaf_nodes=90, warm_start=False; total time=   0.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.9s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   1.0s\n",
      "[CV] END l2_regularization=100, learning_rate=0.1, loss=auto, max_iter=100, max_leaf_nodes=31, warm_start=False; total time=   0.9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=HistGradientBoostingClassifier(), n_iter=100,\n",
       "                   param_distributions={&#x27;l2_regularization&#x27;: [0, 1, 5, 10, 50,\n",
       "                                                              100],\n",
       "                                        &#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        &#x27;loss&#x27;: [&#x27;log_loss&#x27;, &#x27;auto&#x27;,\n",
       "                                                 &#x27;binary_crossentropyv&#x27;,\n",
       "                                                 &#x27;categorical_crossentropy&#x27;],\n",
       "                                        &#x27;max_iter&#x27;: [100, 1000, 10000],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [1, 15, 31, 65, 90,\n",
       "                                                           150],\n",
       "                                        &#x27;warm_start&#x27;: [False]},\n",
       "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=HistGradientBoostingClassifier(), n_iter=100,\n",
       "                   param_distributions={&#x27;l2_regularization&#x27;: [0, 1, 5, 10, 50,\n",
       "                                                              100],\n",
       "                                        &#x27;learning_rate&#x27;: [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        &#x27;loss&#x27;: [&#x27;log_loss&#x27;, &#x27;auto&#x27;,\n",
       "                                                 &#x27;binary_crossentropyv&#x27;,\n",
       "                                                 &#x27;categorical_crossentropy&#x27;],\n",
       "                                        &#x27;max_iter&#x27;: [100, 1000, 10000],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [1, 15, 31, 65, 90,\n",
       "                                                           150],\n",
       "                                        &#x27;warm_start&#x27;: [False]},\n",
       "                   verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=HistGradientBoostingClassifier(), n_iter=100,\n",
       "                   param_distributions={'l2_regularization': [0, 1, 5, 10, 50,\n",
       "                                                              100],\n",
       "                                        'learning_rate': [0.0001, 0.001, 0.01,\n",
       "                                                          0.1, 1, 10, 100,\n",
       "                                                          1000],\n",
       "                                        'loss': ['log_loss', 'auto',\n",
       "                                                 'binary_crossentropyv',\n",
       "                                                 'categorical_crossentropy'],\n",
       "                                        'max_iter': [100, 1000, 10000],\n",
       "                                        'max_leaf_nodes': [1, 15, 31, 65, 90,\n",
       "                                                           150],\n",
       "                                        'warm_start': [False]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parametros: {'warm_start': False, 'max_leaf_nodes': 15, 'max_iter': 10000, 'loss': 'log_loss', 'learning_rate': 0.001, 'l2_regularization': 50}\n",
      "Mejor Score: 0.6686542891990784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Mejores parametros: \"+str(model.best_params_))\n",
    "print(\"Mejor Score: \"+str(model.best_score_)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_warm_start</th>\n",
       "      <th>param_max_leaf_nodes</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_l2_regularization</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.727133</td>\n",
       "      <td>4.860169</td>\n",
       "      <td>0.817556</td>\n",
       "      <td>0.225269</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>10000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>{'warm_start': False, 'max_leaf_nodes': 15, 'max_iter': 10000, 'loss': 'log_loss', 'learning_rate': 0.001, 'l2_regularization': 50}</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>0.664544</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.668654</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.044491</td>\n",
       "      <td>0.007725</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>100</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'warm_start': False, 'max_leaf_nodes': 31, 'max_iter': 100, 'loss': 'auto', 'learning_rate': 0.1, 'l2_regularization': 100}</td>\n",
       "      <td>0.658174</td>\n",
       "      <td>0.679406</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.668085</td>\n",
       "      <td>0.665676</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.092625</td>\n",
       "      <td>0.053152</td>\n",
       "      <td>0.010572</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>False</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>{'warm_start': False, 'max_leaf_nodes': 90, 'max_iter': 100, 'loss': 'log_loss', 'learning_rate': 0.01, 'l2_regularization': 50}</td>\n",
       "      <td>0.636943</td>\n",
       "      <td>0.656051</td>\n",
       "      <td>0.651805</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.665957</td>\n",
       "      <td>0.657183</td>\n",
       "      <td>0.012962</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>215.548609</td>\n",
       "      <td>3.598713</td>\n",
       "      <td>0.739096</td>\n",
       "      <td>0.219530</td>\n",
       "      <td>False</td>\n",
       "      <td>150</td>\n",
       "      <td>10000</td>\n",
       "      <td>auto</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>{'warm_start': False, 'max_leaf_nodes': 150, 'max_iter': 10000, 'loss': 'auto', 'learning_rate': 0.0001, 'l2_regularization': 0}</td>\n",
       "      <td>0.626327</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>0.670913</td>\n",
       "      <td>0.675159</td>\n",
       "      <td>0.663830</td>\n",
       "      <td>0.655908</td>\n",
       "      <td>0.018397</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.128058</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.008776</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>False</td>\n",
       "      <td>65</td>\n",
       "      <td>100</td>\n",
       "      <td>auto</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "      <td>{'warm_start': False, 'max_leaf_nodes': 65, 'max_iter': 100, 'loss': 'auto', 'learning_rate': 1000, 'l2_regularization': 50}</td>\n",
       "      <td>0.649682</td>\n",
       "      <td>0.624204</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.657447</td>\n",
       "      <td>0.652933</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "4       68.727133      4.860169         0.817556        0.225269   \n",
       "99       1.044491      0.007725         0.012497        0.006249   \n",
       "63       1.092625      0.053152         0.010572        0.001017   \n",
       "65     215.548609      3.598713         0.739096        0.219530   \n",
       "7        0.128058      0.004869         0.008776        0.001162   \n",
       "\n",
       "   param_warm_start param_max_leaf_nodes param_max_iter param_loss  \\\n",
       "4             False                   15          10000   log_loss   \n",
       "99            False                   31            100       auto   \n",
       "63            False                   90            100   log_loss   \n",
       "65            False                  150          10000       auto   \n",
       "7             False                   65            100       auto   \n",
       "\n",
       "   param_learning_rate param_l2_regularization  \\\n",
       "4                0.001                      50   \n",
       "99                 0.1                     100   \n",
       "63                0.01                      50   \n",
       "65              0.0001                       0   \n",
       "7                 1000                      50   \n",
       "\n",
       "                                                                                                                                 params  \\\n",
       "4   {'warm_start': False, 'max_leaf_nodes': 15, 'max_iter': 10000, 'loss': 'log_loss', 'learning_rate': 0.001, 'l2_regularization': 50}   \n",
       "99         {'warm_start': False, 'max_leaf_nodes': 31, 'max_iter': 100, 'loss': 'auto', 'learning_rate': 0.1, 'l2_regularization': 100}   \n",
       "63     {'warm_start': False, 'max_leaf_nodes': 90, 'max_iter': 100, 'loss': 'log_loss', 'learning_rate': 0.01, 'l2_regularization': 50}   \n",
       "65     {'warm_start': False, 'max_leaf_nodes': 150, 'max_iter': 10000, 'loss': 'auto', 'learning_rate': 0.0001, 'l2_regularization': 0}   \n",
       "7          {'warm_start': False, 'max_leaf_nodes': 65, 'max_iter': 100, 'loss': 'auto', 'learning_rate': 1000, 'l2_regularization': 50}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "4            0.660297           0.662420           0.664544   \n",
       "99           0.658174           0.679406           0.647558   \n",
       "63           0.636943           0.656051           0.651805   \n",
       "65           0.626327           0.643312           0.670913   \n",
       "7            0.649682           0.624204           0.666667   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "4            0.675159           0.680851         0.668654        0.007958   \n",
       "99           0.675159           0.668085         0.665676        0.011565   \n",
       "63           0.675159           0.665957         0.657183        0.012962   \n",
       "65           0.675159           0.663830         0.655908        0.018397   \n",
       "7            0.666667           0.657447         0.652933        0.015707   \n",
       "\n",
       "    rank_test_score  \n",
       "4                 1  \n",
       "99                2  \n",
       "63                3  \n",
       "65                4  \n",
       "7                 5  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veamos los resultados obtenidos\n",
    "scores = pd.DataFrame(model.cv_results_)\n",
    "scores.sort_values(\"mean_test_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ejecuta la predicción de resultados con X_test\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6706281833616299\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[178 109]\n",
      " [ 85 217]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,predictions)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier is: 0.6706281833616299\n",
      "Precision Score of the classifier is: 0.6656441717791411\n",
      "Recall Score of the classifier is: 0.7185430463576159\n",
      "F1 Score of the classifier is: 0.6910828025477708\n",
      "AUC for our classifier is: 0.7368588042550246\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(f\"Accuracy of the classifier is: {accuracy_score(y_test, predictions)}\")\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "print(f\"Precision Score of the classifier is: {precision_score(y_test, predictions)}\")\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(f\"Recall Score of the classifier is: {recall_score(y_test, predictions)}\")\n",
    "\n",
    "# Calculo del F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f\"F1 Score of the classifier is: {f1_score(y_test, predictions)}\")\n",
    "\n",
    "#Template CURVA - ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "class_probabilities = model.predict_proba(X_test)\n",
    "preds = class_probabilities[:, 1]\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# AUC\n",
    "print(f\"AUC for our classifier is: {roc_auc}\")\n",
    "\n",
    "# Gráfica de la Curva ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Al modelo le cuesta detectar las clases (hay mucho falsos positivos y negativos). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
